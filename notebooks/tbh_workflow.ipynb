{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d73cae0",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d4eec",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b9e61e-5776-4733-bd9a-fd742c4c754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs – from Nuo\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c17899",
   "metadata": {},
   "source": [
    "## Load traning and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f77f8e",
   "metadata": {},
   "source": [
    "### Full-length transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "21781f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proc/training_dataset.pkl\", \"rb\") as f:\n",
    "    training_dataset = pickle.load(f)\n",
    "\n",
    "with open(\"proc/test_dataset.pkl\", \"rb\") as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f1799",
   "metadata": {},
   "source": [
    "### 80nt transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aab20be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proc/training_dataset_80nt.pkl\", \"rb\") as f:\n",
    "    training_dataset = pickle.load(f)\n",
    "\n",
    "with open(\"proc/test_dataset_80nt.pkl\", \"rb\") as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28c975",
   "metadata": {},
   "source": [
    "### Define NtDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "349564d3-b39b-46b7-919f-2854fb2958df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset – from Hannah and Lucas\n",
    "\n",
    "class NtDataset:\n",
    "    \"\"\"Nucleotide sequence + splice sites dataset.\"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.map = {'A':0, 'G':1, 'C':2, 'T':3}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        nt_seq = self.dataset[idx][0]\n",
    "        strengths = self.dataset[idx][1].type(torch.LongTensor)\n",
    "        \n",
    "        tokenized_seq = []\n",
    "        \n",
    "        for letter in nt_seq:\n",
    "            tokenized_seq.append(self.map[letter])\n",
    "            \n",
    "        return torch.tensor(tokenized_seq), torch.tensor(strengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58006e4f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2f518114-428d-4a1b-8ddd-593eaa447046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import optim\n",
    "\n",
    "class SpliceFormer(nn.Module):\n",
    "    \"\"\"Transformer for splice site prediction\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size:int,\n",
    "        model_dim: int,\n",
    "        n_attn_heads: int,\n",
    "        n_encoder_layers: int,\n",
    "        hidden_act: Module,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=n_attn_heads,\n",
    "            dim_feedforward=model_dim,\n",
    "            dropout=dropout,\n",
    "            activation=hidden_act,\n",
    "            batch_first=True)\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim=model_dim)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=self.encoder_layer, num_layers=n_encoder_layers)\n",
    "\n",
    "        self.out_layer = nn.Linear(in_features=model_dim, out_features=3, bias=False)\n",
    "        # self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #self.vocab_size,\n",
    "        x_emb = self.embedding(inputs)\n",
    "\n",
    "        # inputs: (batch_size, seq_len, n_tokens)\n",
    "        encoder_output = self.encoder(x_emb)\n",
    "        outputs = self.out_layer(encoder_output)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5eb54b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ef70f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_subset = training_dataset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "53ab310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleotide_loader = NtDataset(training_dataset_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b6a28000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.NtDataset"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nucleotide_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleotide_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e6823382",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseq_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnucleotide_seq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnucleotide_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnucleotide_seq\u001b[49m\n",
      "Cell \u001b[0;32mIn[99], line 14\u001b[0m, in \u001b[0;36mNtDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     13\u001b[0m     nt_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m     strengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m(torch\u001b[38;5;241m.\u001b[39mLongTensor)\n\u001b[1;32m     16\u001b[0m     tokenized_seq \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m letter \u001b[38;5;129;01min\u001b[39;00m nt_seq:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "for seq_number, nucleotide_seq in enumerate(nucleotide_loader):\n",
    "    inputs, labels = nucleotide_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3155d762-4a17-42f5-a9e3-c9668f3f9b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5741e-01,  5.7796e-01,  3.6048e-01],\n",
      "        [ 5.3551e-02,  5.4709e-01, -2.5090e-01],\n",
      "        [ 1.1731e-01,  8.0545e-02, -2.5017e-01],\n",
      "        [ 1.0592e+00, -3.9247e-02,  4.1319e-01],\n",
      "        [ 1.0845e+00, -1.6636e-02,  3.6382e-01],\n",
      "        [ 9.8309e-01,  3.0090e-02,  4.6850e-01],\n",
      "        [ 1.0638e+00,  1.0154e-02,  6.0749e-01],\n",
      "        [-1.7633e-01,  4.3838e-01, -4.7099e-01],\n",
      "        [ 1.2359e+00,  1.4398e-01,  5.2272e-01],\n",
      "        [ 9.8833e-02,  6.2473e-01, -2.7987e-01],\n",
      "        [ 1.2760e-01,  4.7684e-01, -1.7856e-01],\n",
      "        [ 1.0098e+00,  4.8979e-02,  3.7940e-01],\n",
      "        [ 7.4366e-02,  2.0094e-01, -2.2009e-01],\n",
      "        [-5.1121e-02, -1.8168e-02, -1.5731e-01],\n",
      "        [ 9.0506e-02,  6.6663e-01, -2.2833e-01],\n",
      "        [-1.6358e-01,  5.9888e-01, -2.6920e-01],\n",
      "        [ 1.0553e-01,  7.4435e-01, -3.6213e-01],\n",
      "        [-5.3668e-04,  5.2576e-01, -5.5156e-01],\n",
      "        [-3.8144e-01,  6.6507e-01,  1.4273e-01],\n",
      "        [-1.8656e-01,  4.3773e-01, -1.1563e-01],\n",
      "        [-7.1222e-02,  5.0095e-01, -2.6393e-01],\n",
      "        [ 8.7866e-02,  5.1764e-01, -2.1446e-01],\n",
      "        [ 1.1192e+00,  5.4022e-02,  5.5371e-01],\n",
      "        [ 1.0262e+00,  1.4117e-01,  6.4743e-01],\n",
      "        [ 1.0555e-01,  4.4686e-01, -4.7193e-01],\n",
      "        [ 1.0609e+00, -8.9957e-02,  4.5902e-01],\n",
      "        [-5.1990e-02,  5.2301e-01, -4.7155e-01],\n",
      "        [ 1.1113e+00,  7.1121e-02,  4.6363e-01],\n",
      "        [ 1.0839e+00,  8.3437e-02,  4.0541e-01],\n",
      "        [-1.7767e-01,  7.8759e-01,  2.3669e-01],\n",
      "        [ 2.5500e-02,  7.1084e-01, -7.1551e-02],\n",
      "        [ 1.2050e-01, -1.9522e-02, -1.5179e-01],\n",
      "        [-7.3201e-02,  2.2096e-01, -8.8408e-02],\n",
      "        [ 5.7647e-02,  6.8461e-01, -2.1445e-01],\n",
      "        [ 1.0279e+00, -3.7302e-02,  4.6271e-01],\n",
      "        [ 1.0596e+00, -3.3697e-02,  6.2657e-01],\n",
      "        [-2.5015e-01,  7.1386e-01,  2.8385e-01],\n",
      "        [-2.5360e-01,  6.7459e-01,  2.8404e-01],\n",
      "        [-1.0130e-01,  5.2500e-01, -2.4157e-01],\n",
      "        [-4.1046e-02,  4.6239e-01, -1.9672e-01],\n",
      "        [ 1.1135e+00, -7.8219e-02,  3.5018e-01],\n",
      "        [ 1.1523e+00, -1.2114e-02,  4.7905e-01],\n",
      "        [ 1.4854e-02,  4.6246e-01, -1.8070e-01],\n",
      "        [-2.8668e-02,  6.2609e-01, -3.0670e-01],\n",
      "        [-2.2511e-01,  4.5483e-01, -4.5173e-01],\n",
      "        [ 1.9590e-01,  1.9625e-01, -2.3779e-01],\n",
      "        [ 9.4615e-01,  7.3420e-02,  5.3025e-01],\n",
      "        [ 1.0238e+00, -9.7955e-02,  4.4078e-01],\n",
      "        [ 1.1667e+00,  4.9269e-02,  6.1631e-01],\n",
      "        [-2.8963e-01,  6.1921e-01,  3.7300e-01],\n",
      "        [-8.7483e-02,  4.5130e-01, -5.1396e-01],\n",
      "        [ 1.2174e-01,  6.5921e-02, -1.9371e-01],\n",
      "        [ 9.5185e-01, -2.8687e-02,  2.7315e-01],\n",
      "        [-2.5340e-01,  7.0986e-01,  1.9086e-01],\n",
      "        [-5.7955e-02,  4.3710e-01, -2.6669e-01],\n",
      "        [ 1.2045e-02,  3.9537e-02, -2.1483e-01],\n",
      "        [ 9.0486e-01,  5.3816e-02,  4.3887e-01],\n",
      "        [ 9.7259e-01,  4.5348e-02,  4.0784e-01],\n",
      "        [-1.8009e-01,  3.9053e-01, -3.6696e-01],\n",
      "        [ 8.4216e-02,  9.9022e-02, -9.3291e-02],\n",
      "        [-1.9277e-01,  4.5073e-01, -3.4451e-01],\n",
      "        [ 1.4136e-01, -2.6467e-03, -1.6108e-01],\n",
      "        [-1.8035e-01,  5.9601e-01,  4.8856e-01],\n",
      "        [-1.0029e-01,  4.1187e-01, -4.4333e-01],\n",
      "        [ 1.6215e-01,  6.8542e-01, -2.0015e-01],\n",
      "        [-7.2824e-02,  6.5644e-01,  3.5005e-01],\n",
      "        [-2.1933e-01,  3.5676e-01, -5.1765e-01],\n",
      "        [ 1.0359e-01,  1.0152e-01, -3.0317e-01],\n",
      "        [ 1.2867e-02,  5.1101e-01, -1.4033e-01],\n",
      "        [ 1.1142e+00,  3.6255e-02,  4.2895e-01],\n",
      "        [-9.2315e-02,  6.3871e-01,  3.3570e-01],\n",
      "        [ 1.0378e+00, -6.9493e-02,  4.2183e-01],\n",
      "        [-2.9239e-01,  5.6017e-01, -2.0063e-01],\n",
      "        [ 9.8144e-01, -1.3757e-02,  4.3751e-01],\n",
      "        [ 3.0252e-01,  2.9801e-01,  2.0893e-02],\n",
      "        [ 9.7174e-01,  4.0339e-02,  6.3461e-01],\n",
      "        [-1.8330e-01,  2.4608e-01, -2.9664e-01],\n",
      "        [ 2.0536e-02,  6.3019e-01, -2.7750e-01],\n",
      "        [ 1.8737e-02,  7.6100e-02, -2.8873e-01],\n",
      "        [ 1.7102e-01,  3.9575e-01, -1.6230e-01]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Half",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(labels)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/project/s/shreejoy/nxu/envs/patch_seq_spl/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/s/shreejoy/nxu/envs/patch_seq_spl/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/project/s/shreejoy/nxu/envs/patch_seq_spl/lib/python3.12/site-packages/torch/nn/modules/loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/s/shreejoy/nxu/envs/patch_seq_spl/lib/python3.12/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Half"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "n_epochs = 2\n",
    "nucleotide_loader = NtDataset(training_dataset_subset)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "splice_model = SpliceFormer(vocab_size=4, \n",
    "                            model_dim=64, \n",
    "                            n_attn_heads=2, \n",
    "                            n_encoder_layers=2, \n",
    "                            hidden_act=nn.ReLU(), \n",
    "                            dropout=0.1)\n",
    "\n",
    "optimizer = torch.optim.SGD(splice_model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# training loop \n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for seq_number, nucleotide_seq in enumerate(nucleotide_loader):\n",
    "        inputs, labels = nucleotide_seq\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # error is here!\n",
    "        outputs = splice_model(inputs)\n",
    "        print(outputs)\n",
    "        # print(labels)\n",
    "\n",
    "        total_loss = loss_fn(outputs, labels)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "        if seq_number % 10 == 0:\n",
    "            print(f'epoch: {epoch}, step: {seq_number}, loss: {running_loss}')\n",
    "            torch.save(splice_model.state_dict(), f'./tbh_model_{seq_number}.pth')\n",
    "            # running_loss = 0.0\n",
    "\n",
    "            \n",
    "print(\"Finished training!\\nFinal loss value:\", total_loss)\n",
    "torch.save(splice_model.state_dict(), './tbh_model_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "618b7f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 3]) torch.Size([80])\n",
      "tensor(1.2545, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for seq_number, nucleotide_seq in enumerate(nucleotide_loader):\n",
    "    inputs, labels = nucleotide_seq\n",
    "    labels = labels.reshape(-1)\n",
    "    # labels = labels.unsqueeze(0)\n",
    "    break\n",
    "\n",
    "outputs = splice_model(inputs)\n",
    "labels = labels.type(torch.LongTensor)\n",
    "# outputs = outputs.unsqueeze(0)\n",
    "print(outputs.shape, labels.shape)\n",
    "total_loss = loss_fn(outputs, labels)\n",
    "print(total_loss)\n",
    "# print(outputs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ddf0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd200d59-6665-487f-80ff-b8e292d97c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "nucleotide_test_loader = NtDataset(test_dataset)\n",
    "splice_test = SpliceFormer()\n",
    "splice_test.load_state_dict(torch.load('./tbh_model_final.pth', weights_only = True)) \n",
    "\n",
    "outputs = splice_test.(nucleotide_seq)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
