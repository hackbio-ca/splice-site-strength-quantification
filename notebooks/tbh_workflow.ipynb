{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d73cae0",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d4eec",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b9e61e-5776-4733-bd9a-fd742c4c754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs – from Nuo\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c17899",
   "metadata": {},
   "source": [
    "## Load traning and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f77f8e",
   "metadata": {},
   "source": [
    "### Full-length transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "21781f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proc/training_dataset.pkl\", \"rb\") as f:\n",
    "    training_dataset = pickle.load(f)\n",
    "\n",
    "with open(\"proc/test_dataset.pkl\", \"rb\") as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f1799",
   "metadata": {},
   "source": [
    "### 80nt transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab20be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proc/training_dataset_80nt.pkl\", \"rb\") as f:\n",
    "    training_dataset = pickle.load(f)\n",
    "\n",
    "with open(\"proc/test_dataset_80nt.pkl\", \"rb\") as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28c975",
   "metadata": {},
   "source": [
    "### Define NtDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349564d3-b39b-46b7-919f-2854fb2958df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset – from Hannah and Lucas\n",
    "\n",
    "class NtDataset:\n",
    "    \"\"\"Nucleotide sequence + splice sites dataset.\"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.map = {'A':0, 'G':1, 'C':2, 'T':3}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        nt_seq = self.dataset[idx][0]\n",
    "        strengths = self.dataset[idx][1]\n",
    "        \n",
    "        tokenized_seq = []\n",
    "        \n",
    "        for letter in nt_seq:\n",
    "            tokenized_seq.append(self.map[letter])\n",
    "            \n",
    "        return torch.tensor(tokenized_seq), torch.tensor(strengths).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58006e4f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f518114-428d-4a1b-8ddd-593eaa447046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import optim\n",
    "\n",
    "class SpliceFormer(nn.Module):\n",
    "    \"\"\"Transformer for splice site prediction\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size:int,\n",
    "        model_dim: int,\n",
    "        n_attn_heads: int,\n",
    "        n_encoder_layers: int,\n",
    "        hidden_act: Module,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=n_attn_heads,\n",
    "            dim_feedforward=model_dim,\n",
    "            dropout=dropout,\n",
    "            activation=hidden_act,\n",
    "            batch_first=True)\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim=model_dim)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=self.encoder_layer, num_layers=n_encoder_layers)\n",
    "\n",
    "        self.out_layer = nn.Linear(in_features=model_dim, out_features=3, bias=False)\n",
    "        # self.lm_head.weight = self.embedding.weight\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #self.vocab_size,\n",
    "        x_emb = self.embedding(inputs)\n",
    "\n",
    "        # inputs: (batch_size, seq_len, n_tokens)\n",
    "        encoder_output = self.encoder(x_emb)\n",
    "        outputs = self.out_layer(encoder_output)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5eb54b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef70f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_subset = training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3155d762-4a17-42f5-a9e3-c9668f3f9b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 0.9895215034484863\n",
      "epoch: 0, step: 10, loss: 9.94553828239441\n",
      "epoch: 0, step: 20, loss: 17.673371195793152\n",
      "epoch: 0, step: 30, loss: 24.22443813085556\n",
      "epoch: 0, step: 40, loss: 29.779487371444702\n",
      "epoch: 0, step: 50, loss: 34.64477500319481\n",
      "epoch: 0, step: 60, loss: 38.85314616560936\n",
      "epoch: 0, step: 70, loss: 42.7998731136322\n",
      "epoch: 0, step: 80, loss: 45.94551703333855\n",
      "epoch: 0, step: 90, loss: 48.873563677072525\n",
      "epoch: 0, step: 100, loss: 51.956222623586655\n",
      "epoch: 0, step: 110, loss: 54.78481739759445\n",
      "epoch: 0, step: 120, loss: 57.238715410232544\n",
      "epoch: 0, step: 130, loss: 59.47136886417866\n",
      "epoch: 0, step: 140, loss: 61.52211035788059\n",
      "epoch: 0, step: 150, loss: 63.50655588507652\n",
      "epoch: 0, step: 160, loss: 65.36693830788136\n",
      "epoch: 0, step: 170, loss: 67.20206490159035\n",
      "epoch: 0, step: 180, loss: 68.91383616626263\n",
      "epoch: 0, step: 190, loss: 70.85664342343807\n",
      "epoch: 0, step: 200, loss: 72.75173152983189\n",
      "epoch: 0, step: 210, loss: 74.26308397948742\n",
      "epoch: 0, step: 220, loss: 75.84979996085167\n",
      "epoch: 0, step: 230, loss: 77.45849817246199\n",
      "epoch: 0, step: 240, loss: 80.02277655154467\n",
      "epoch: 0, step: 250, loss: 81.72659262269735\n",
      "epoch: 0, step: 260, loss: 83.30456575751305\n",
      "epoch: 0, step: 270, loss: 84.61592761427164\n",
      "epoch: 0, step: 280, loss: 86.0188010185957\n",
      "epoch: 0, step: 290, loss: 87.41156702488661\n",
      "epoch: 0, step: 300, loss: 88.74104283004999\n",
      "epoch: 0, step: 310, loss: 90.45331957936287\n",
      "epoch: 0, step: 320, loss: 91.80715458840132\n",
      "epoch: 0, step: 330, loss: 93.20476406812668\n",
      "epoch: 0, step: 340, loss: 94.43000261485577\n",
      "epoch: 0, step: 350, loss: 95.7119863703847\n",
      "epoch: 0, step: 360, loss: 97.0130640566349\n",
      "epoch: 0, step: 370, loss: 98.28956608474255\n",
      "epoch: 0, step: 380, loss: 99.54842535406351\n",
      "epoch: 0, step: 390, loss: 101.28753066807985\n",
      "epoch: 0, step: 400, loss: 103.38460526615381\n",
      "epoch: 0, step: 410, loss: 104.64207949489355\n",
      "epoch: 0, step: 420, loss: 106.02567469328642\n",
      "epoch: 0, step: 430, loss: 107.74423132836819\n",
      "epoch: 0, step: 440, loss: 109.10564179718494\n",
      "epoch: 0, step: 450, loss: 110.52896593511105\n",
      "epoch: 0, step: 460, loss: 111.88615673035383\n",
      "epoch: 0, step: 470, loss: 113.5090157687664\n",
      "epoch: 0, step: 480, loss: 114.94920825213194\n",
      "epoch: 0, step: 490, loss: 116.19941277056932\n",
      "epoch: 0, step: 500, loss: 117.4204218685627\n",
      "epoch: 0, step: 510, loss: 118.59529852867126\n",
      "epoch: 0, step: 520, loss: 120.06692549586296\n",
      "epoch: 0, step: 530, loss: 121.33393615484238\n",
      "epoch: 0, step: 540, loss: 122.84353890269995\n",
      "epoch: 0, step: 550, loss: 124.332623206079\n",
      "epoch: 0, step: 560, loss: 125.62784159183502\n",
      "epoch: 0, step: 570, loss: 127.5823667794466\n",
      "epoch: 0, step: 580, loss: 128.50904248654842\n",
      "epoch: 0, step: 590, loss: 129.9295396208763\n",
      "epoch: 0, step: 600, loss: 131.14157477766275\n",
      "epoch: 0, step: 610, loss: 132.50036270916462\n",
      "epoch: 0, step: 620, loss: 134.06806238740683\n",
      "epoch: 0, step: 630, loss: 135.14793782681227\n",
      "epoch: 0, step: 640, loss: 136.36133339256048\n",
      "epoch: 0, step: 650, loss: 137.2885950356722\n",
      "epoch: 0, step: 660, loss: 138.28074549883604\n",
      "epoch: 0, step: 670, loss: 139.92782339453697\n",
      "epoch: 0, step: 680, loss: 142.28263986110687\n",
      "epoch: 0, step: 690, loss: 145.63030438870192\n",
      "epoch: 0, step: 700, loss: 146.8739577755332\n",
      "epoch: 0, step: 710, loss: 148.2210581973195\n",
      "epoch: 0, step: 720, loss: 149.5452963784337\n",
      "epoch: 0, step: 730, loss: 150.55335339158773\n",
      "epoch: 0, step: 740, loss: 152.0922140404582\n",
      "epoch: 0, step: 750, loss: 153.3780058324337\n",
      "epoch: 0, step: 760, loss: 154.78848332166672\n",
      "epoch: 0, step: 770, loss: 156.04130472242832\n",
      "epoch: 0, step: 780, loss: 157.29362475126982\n",
      "epoch: 0, step: 790, loss: 158.6018433868885\n",
      "epoch: 0, step: 800, loss: 160.02390477061272\n",
      "epoch: 0, step: 810, loss: 161.4759830608964\n",
      "epoch: 0, step: 820, loss: 163.6551250293851\n",
      "epoch: 0, step: 830, loss: 165.74924168735743\n",
      "epoch: 0, step: 840, loss: 166.76193989813328\n",
      "epoch: 0, step: 850, loss: 167.83814300596714\n",
      "epoch: 0, step: 860, loss: 168.92609658837318\n",
      "epoch: 0, step: 870, loss: 170.58845750242472\n",
      "epoch: 0, step: 880, loss: 172.17392516881227\n",
      "epoch: 0, step: 890, loss: 173.42657461762428\n",
      "epoch: 0, step: 900, loss: 174.82735385000706\n",
      "epoch: 0, step: 910, loss: 176.53743325173855\n",
      "epoch: 0, step: 920, loss: 177.75673358887434\n",
      "epoch: 0, step: 930, loss: 178.90261375904083\n",
      "epoch: 0, step: 940, loss: 180.1716925650835\n",
      "epoch: 0, step: 950, loss: 181.90783169865608\n",
      "epoch: 0, step: 960, loss: 183.34831298142672\n",
      "epoch: 0, step: 970, loss: 184.19502768665552\n",
      "epoch: 0, step: 980, loss: 185.40955401211977\n",
      "epoch: 0, step: 990, loss: 186.5718597099185\n",
      "epoch: 0, step: 1000, loss: 187.43229969590902\n",
      "epoch: 0, step: 1010, loss: 189.00199180096388\n",
      "epoch: 0, step: 1020, loss: 190.15591889619827\n",
      "epoch: 0, step: 1030, loss: 191.7967755421996\n",
      "epoch: 0, step: 1040, loss: 192.71304398030043\n",
      "epoch: 0, step: 1050, loss: 193.81071869283915\n",
      "epoch: 0, step: 1060, loss: 195.0592906475067\n",
      "epoch: 0, step: 1070, loss: 196.21266496181488\n",
      "epoch: 0, step: 1080, loss: 197.51607099175453\n",
      "epoch: 0, step: 1090, loss: 198.67408014833927\n",
      "epoch: 0, step: 1100, loss: 199.8188752681017\n",
      "epoch: 0, step: 1110, loss: 200.9743062481284\n",
      "epoch: 0, step: 1120, loss: 202.21308348327875\n",
      "epoch: 0, step: 1130, loss: 203.27038061618805\n",
      "epoch: 0, step: 1140, loss: 204.12429136037827\n",
      "epoch: 0, step: 1150, loss: 205.06004136055708\n",
      "epoch: 0, step: 1160, loss: 206.1239274069667\n",
      "epoch: 0, step: 1170, loss: 207.1441690325737\n",
      "epoch: 0, step: 1180, loss: 210.2461773082614\n",
      "epoch: 0, step: 1190, loss: 214.5263833925128\n",
      "epoch: 0, step: 1200, loss: 216.3751349002123\n",
      "epoch: 0, step: 1210, loss: 217.44707350432873\n",
      "epoch: 0, step: 1220, loss: 218.77236369997263\n",
      "epoch: 0, step: 1230, loss: 220.3982214257121\n",
      "epoch: 0, step: 1240, loss: 221.551748290658\n",
      "epoch: 0, step: 1250, loss: 222.76189229637384\n",
      "epoch: 0, step: 1260, loss: 225.68229123204947\n",
      "epoch: 0, step: 1270, loss: 226.83254259824753\n",
      "epoch: 0, step: 1280, loss: 228.0105987340212\n",
      "epoch: 0, step: 1290, loss: 229.2631260678172\n",
      "epoch: 0, step: 1300, loss: 230.29787684231997\n",
      "epoch: 0, step: 1310, loss: 231.5408910214901\n",
      "epoch: 0, step: 1320, loss: 233.0649240091443\n",
      "epoch: 0, step: 1330, loss: 234.14368564635515\n",
      "epoch: 0, step: 1340, loss: 235.03030082583427\n",
      "epoch: 0, step: 1350, loss: 236.55658820271492\n",
      "epoch: 0, step: 1360, loss: 237.96750029176474\n",
      "epoch: 0, step: 1370, loss: 239.5494009628892\n",
      "epoch: 0, step: 1380, loss: 240.60100808739662\n",
      "epoch: 0, step: 1390, loss: 241.92838103324175\n",
      "epoch: 0, step: 1400, loss: 243.26005224138498\n",
      "epoch: 0, step: 1410, loss: 244.25421649217606\n",
      "epoch: 0, step: 1420, loss: 245.9614096134901\n",
      "epoch: 0, step: 1430, loss: 247.94850915670395\n",
      "epoch: 0, step: 1440, loss: 249.11080972105265\n",
      "epoch: 0, step: 1450, loss: 250.41742181032896\n",
      "epoch: 0, step: 1460, loss: 251.3963991254568\n",
      "epoch: 0, step: 1470, loss: 252.75109557807446\n",
      "epoch: 0, step: 1480, loss: 253.62481562048197\n",
      "epoch: 0, step: 1490, loss: 254.87908565998077\n",
      "epoch: 0, step: 1500, loss: 255.85500399023294\n",
      "epoch: 0, step: 1510, loss: 257.21658784896135\n",
      "epoch: 0, step: 1520, loss: 258.2366390749812\n",
      "epoch: 0, step: 1530, loss: 259.54843678325415\n",
      "epoch: 0, step: 1540, loss: 260.70127066224813\n",
      "epoch: 0, step: 1550, loss: 261.5535889416933\n",
      "epoch: 0, step: 1560, loss: 262.48120994865894\n",
      "epoch: 0, step: 1570, loss: 263.37583611160517\n",
      "epoch: 0, step: 1580, loss: 264.37631165236235\n",
      "epoch: 0, step: 1590, loss: 265.5397745370865\n",
      "epoch: 0, step: 1600, loss: 266.77557042986155\n",
      "epoch: 0, step: 1610, loss: 267.87929785996675\n",
      "epoch: 0, step: 1620, loss: 269.20013562589884\n",
      "epoch: 0, step: 1630, loss: 270.28149392455816\n",
      "epoch: 0, step: 1640, loss: 271.5150797665119\n",
      "epoch: 0, step: 1650, loss: 272.5028856471181\n",
      "epoch: 0, step: 1660, loss: 273.66390286386013\n",
      "epoch: 0, step: 1670, loss: 274.9496214464307\n",
      "epoch: 0, step: 1680, loss: 275.9320464283228\n",
      "epoch: 0, step: 1690, loss: 277.0610278323293\n",
      "epoch: 0, step: 1700, loss: 278.01407957822084\n",
      "epoch: 0, step: 1710, loss: 279.0009070560336\n",
      "epoch: 0, step: 1720, loss: 280.1139880269766\n",
      "epoch: 0, step: 1730, loss: 281.0114864036441\n",
      "epoch: 0, step: 1740, loss: 282.07660902291536\n",
      "epoch: 0, step: 1750, loss: 283.3110973984003\n",
      "epoch: 0, step: 1760, loss: 285.38231417536736\n",
      "epoch: 0, step: 1770, loss: 286.29740084707737\n",
      "epoch: 0, step: 1780, loss: 287.04958363622427\n",
      "epoch: 0, step: 1790, loss: 288.0024252384901\n",
      "epoch: 0, step: 1800, loss: 288.9718729108572\n",
      "epoch: 0, step: 1810, loss: 289.9109019637108\n",
      "epoch: 0, step: 1820, loss: 291.0540037751198\n",
      "epoch: 0, step: 1830, loss: 292.2989522963762\n",
      "epoch: 0, step: 1840, loss: 293.2081112489104\n",
      "epoch: 0, step: 1850, loss: 294.73337707668543\n",
      "epoch: 0, step: 1860, loss: 296.0677453801036\n",
      "epoch: 0, step: 1870, loss: 296.9387149810791\n",
      "epoch: 0, step: 1880, loss: 298.3101629242301\n",
      "epoch: 0, step: 1890, loss: 299.43165857344866\n",
      "epoch: 0, step: 1900, loss: 301.08399026095867\n",
      "epoch: 0, step: 1910, loss: 302.02951081842184\n",
      "epoch: 0, step: 1920, loss: 303.2835476845503\n",
      "epoch: 0, step: 1930, loss: 304.6027068644762\n",
      "epoch: 0, step: 1940, loss: 305.8150870203972\n",
      "epoch: 0, step: 1950, loss: 306.8173519298434\n",
      "epoch: 0, step: 1960, loss: 307.750952526927\n",
      "epoch: 0, step: 1970, loss: 308.7027805149555\n",
      "epoch: 0, step: 1980, loss: 309.6557493135333\n",
      "epoch: 0, step: 1990, loss: 310.737161681056\n",
      "epoch: 0, step: 2000, loss: 312.3676560074091\n",
      "epoch: 0, step: 2010, loss: 313.48592057824135\n",
      "epoch: 0, step: 2020, loss: 314.3653037697077\n",
      "epoch: 0, step: 2030, loss: 315.2520700842142\n",
      "epoch: 0, step: 2040, loss: 316.43503753840923\n",
      "epoch: 0, step: 2050, loss: 317.33196088671684\n",
      "epoch: 0, step: 2060, loss: 318.1516437456012\n",
      "epoch: 0, step: 2070, loss: 319.7335617542267\n",
      "epoch: 0, step: 2080, loss: 321.17339589446783\n",
      "epoch: 0, step: 2090, loss: 322.0662284195423\n",
      "epoch: 0, step: 2100, loss: 323.24896551668644\n",
      "epoch: 0, step: 2110, loss: 324.46089611947536\n",
      "epoch: 0, step: 2120, loss: 325.8198453858495\n",
      "epoch: 0, step: 2130, loss: 327.44761515408754\n",
      "epoch: 0, step: 2140, loss: 328.6673645824194\n",
      "epoch: 0, step: 2150, loss: 329.4815276116133\n",
      "epoch: 0, step: 2160, loss: 330.41658498346806\n",
      "epoch: 0, step: 2170, loss: 331.4341480880976\n",
      "epoch: 0, step: 2180, loss: 332.8367229029536\n",
      "epoch: 0, step: 2190, loss: 334.1589633524418\n",
      "epoch: 0, step: 2200, loss: 336.3191824629903\n",
      "epoch: 0, step: 2210, loss: 338.7267496213317\n",
      "epoch: 0, step: 2220, loss: 340.33512388914824\n",
      "epoch: 0, step: 2230, loss: 341.54187554866076\n",
      "epoch: 0, step: 2240, loss: 342.33749348670244\n",
      "epoch: 0, step: 2250, loss: 343.4250623360276\n",
      "epoch: 0, step: 2260, loss: 344.22296734154224\n",
      "epoch: 0, step: 2270, loss: 345.2145575284958\n",
      "epoch: 0, step: 2280, loss: 346.6790857985616\n",
      "epoch: 0, step: 2290, loss: 347.54792062938213\n",
      "epoch: 0, step: 2300, loss: 348.81955433636904\n",
      "epoch: 0, step: 2310, loss: 350.2724035382271\n",
      "epoch: 0, step: 2320, loss: 351.62921134382486\n",
      "epoch: 0, step: 2330, loss: 352.6995498612523\n",
      "epoch: 0, step: 2340, loss: 353.59542279690504\n",
      "epoch: 0, step: 2350, loss: 354.5480685085058\n",
      "epoch: 0, step: 2360, loss: 355.98491244763136\n",
      "epoch: 0, step: 2370, loss: 357.4809054508805\n",
      "epoch: 0, step: 2380, loss: 358.5669079273939\n",
      "epoch: 0, step: 2390, loss: 359.79907012730837\n",
      "epoch: 0, step: 2400, loss: 361.1718731895089\n",
      "epoch: 0, step: 2410, loss: 362.2715324833989\n",
      "epoch: 0, step: 2420, loss: 363.7370550557971\n",
      "epoch: 0, step: 2430, loss: 364.79561623185873\n",
      "epoch: 0, step: 2440, loss: 365.88759353011847\n",
      "epoch: 0, step: 2450, loss: 367.95921316742897\n",
      "epoch: 0, step: 2460, loss: 369.09203419834375\n",
      "epoch: 0, step: 2470, loss: 371.01607812196016\n",
      "epoch: 0, step: 2480, loss: 371.89489141106606\n",
      "epoch: 0, step: 2490, loss: 372.8682673498988\n",
      "epoch: 0, step: 2500, loss: 374.26873145997524\n",
      "epoch: 0, step: 2510, loss: 375.2873772904277\n",
      "epoch: 0, step: 2520, loss: 376.12172401696444\n",
      "epoch: 0, step: 2530, loss: 377.0155823677778\n",
      "epoch: 0, step: 2540, loss: 378.0144838318229\n",
      "epoch: 0, step: 2550, loss: 379.5292865112424\n",
      "epoch: 0, step: 2560, loss: 381.0554021000862\n",
      "epoch: 0, step: 2570, loss: 382.2664473205805\n",
      "epoch: 0, step: 2580, loss: 383.4986748471856\n",
      "epoch: 0, step: 2590, loss: 384.52064430713654\n",
      "epoch: 0, step: 2600, loss: 385.38870383799076\n",
      "epoch: 0, step: 2610, loss: 387.25177600234747\n",
      "epoch: 0, step: 2620, loss: 388.4575095772743\n",
      "epoch: 0, step: 2630, loss: 389.7316415980458\n",
      "epoch: 0, step: 2640, loss: 391.2522491440177\n",
      "epoch: 0, step: 2650, loss: 392.5832772925496\n",
      "epoch: 0, step: 2660, loss: 393.6439564451575\n",
      "epoch: 0, step: 2670, loss: 394.45582795888186\n",
      "epoch: 0, step: 2680, loss: 395.3589567914605\n",
      "epoch: 0, step: 2690, loss: 396.4600069001317\n",
      "epoch: 0, step: 2700, loss: 397.42440039664507\n",
      "epoch: 0, step: 2710, loss: 398.63538102060556\n",
      "epoch: 0, step: 2720, loss: 399.5010864138603\n",
      "epoch: 0, step: 2730, loss: 400.46255169063807\n",
      "epoch: 0, step: 2740, loss: 401.53102833777666\n",
      "epoch: 0, step: 2750, loss: 402.672405987978\n",
      "epoch: 0, step: 2760, loss: 403.99134477972984\n",
      "epoch: 0, step: 2770, loss: 405.2477570846677\n",
      "epoch: 0, step: 2780, loss: 406.14742954075336\n",
      "epoch: 0, step: 2790, loss: 407.16812009364367\n",
      "epoch: 0, step: 2800, loss: 409.6039602383971\n",
      "epoch: 0, step: 2810, loss: 411.22994903475046\n",
      "epoch: 0, step: 2820, loss: 412.30938728898764\n",
      "epoch: 0, step: 2830, loss: 413.29050267487764\n",
      "epoch: 0, step: 2840, loss: 414.34436186403036\n",
      "epoch: 0, step: 2850, loss: 415.62984167039394\n",
      "epoch: 0, step: 2860, loss: 417.4204516932368\n",
      "epoch: 0, step: 2870, loss: 418.483294993639\n",
      "epoch: 0, step: 2880, loss: 419.3175625875592\n",
      "epoch: 0, step: 2890, loss: 420.24894765764475\n",
      "epoch: 0, step: 2900, loss: 421.27132073044777\n",
      "epoch: 0, step: 2910, loss: 422.2415148690343\n",
      "epoch: 0, step: 2920, loss: 423.6582539603114\n",
      "epoch: 0, step: 2930, loss: 425.9035524651408\n",
      "epoch: 0, step: 2940, loss: 427.2079855054617\n",
      "epoch: 0, step: 2950, loss: 429.0609612688422\n",
      "epoch: 0, step: 2960, loss: 430.3446309119463\n",
      "epoch: 0, step: 2970, loss: 431.5565900504589\n",
      "epoch: 0, step: 2980, loss: 433.0561114177108\n",
      "epoch: 0, step: 2990, loss: 434.30191699415445\n",
      "epoch: 0, step: 3000, loss: 436.18031034618616\n",
      "epoch: 0, step: 3010, loss: 437.86954691261053\n",
      "epoch: 0, step: 3020, loss: 439.26496543735266\n",
      "epoch: 0, step: 3030, loss: 440.5899042263627\n",
      "epoch: 0, step: 3040, loss: 442.1866131722927\n",
      "epoch: 0, step: 3050, loss: 443.4099497795105\n",
      "epoch: 0, step: 3060, loss: 444.4647266715765\n",
      "epoch: 0, step: 3070, loss: 445.2834084033966\n",
      "epoch: 0, step: 3080, loss: 446.4744534417987\n",
      "epoch: 0, step: 3090, loss: 447.5388734936714\n",
      "epoch: 0, step: 3100, loss: 448.4873038902879\n",
      "epoch: 0, step: 3110, loss: 449.73433125019073\n",
      "epoch: 0, step: 3120, loss: 450.8312303572893\n",
      "epoch: 0, step: 3130, loss: 451.90395250171423\n",
      "epoch: 0, step: 3140, loss: 453.24958941340446\n",
      "epoch: 0, step: 3150, loss: 454.2392732128501\n",
      "epoch: 0, step: 3160, loss: 455.42938677221537\n",
      "epoch: 0, step: 3170, loss: 457.7543453797698\n",
      "epoch: 0, step: 3180, loss: 459.20507780462503\n",
      "epoch: 0, step: 3190, loss: 460.6418326124549\n",
      "epoch: 0, step: 3200, loss: 461.98367613554\n",
      "epoch: 0, step: 3210, loss: 463.07856894284487\n",
      "epoch: 0, step: 3220, loss: 464.2193214595318\n",
      "epoch: 0, step: 3230, loss: 465.5775598883629\n",
      "epoch: 0, step: 3240, loss: 466.985261939466\n",
      "epoch: 0, step: 3250, loss: 468.2406634762883\n",
      "epoch: 0, step: 3260, loss: 469.13646095991135\n",
      "epoch: 0, step: 3270, loss: 470.27906449884176\n",
      "epoch: 0, step: 3280, loss: 471.632711045444\n",
      "epoch: 0, step: 3290, loss: 472.64888782054186\n",
      "epoch: 0, step: 3300, loss: 474.1089183241129\n",
      "epoch: 0, step: 3310, loss: 475.93173161149025\n",
      "epoch: 0, step: 3320, loss: 477.32999081909657\n",
      "epoch: 0, step: 3330, loss: 478.36666337400675\n",
      "epoch: 0, step: 3340, loss: 479.5391180664301\n",
      "epoch: 0, step: 3350, loss: 481.9114270731807\n",
      "epoch: 0, step: 3360, loss: 483.46979127824306\n",
      "epoch: 0, step: 3370, loss: 484.81831353902817\n",
      "epoch: 0, step: 3380, loss: 485.78668943047523\n",
      "epoch: 0, step: 3390, loss: 486.7986264228821\n",
      "epoch: 0, step: 3400, loss: 489.4926201403141\n",
      "epoch: 0, step: 3410, loss: 490.6085506901145\n",
      "epoch: 0, step: 3420, loss: 491.70567335933447\n",
      "epoch: 0, step: 3430, loss: 492.7914127781987\n",
      "epoch: 0, step: 3440, loss: 494.1594806164503\n",
      "epoch: 0, step: 3450, loss: 495.3419568091631\n",
      "epoch: 0, step: 3460, loss: 496.5030461624265\n",
      "epoch: 0, step: 3470, loss: 497.4229399114847\n",
      "epoch: 0, step: 3480, loss: 498.2697760090232\n",
      "epoch: 0, step: 3490, loss: 500.4842145293951\n",
      "epoch: 0, step: 3500, loss: 501.57433358579874\n",
      "epoch: 0, step: 3510, loss: 502.4441889002919\n",
      "epoch: 0, step: 3520, loss: 503.45331736654043\n",
      "epoch: 0, step: 3530, loss: 504.7654143795371\n",
      "epoch: 0, step: 3540, loss: 506.1640475541353\n",
      "epoch: 0, step: 3550, loss: 507.33259327709675\n",
      "epoch: 0, step: 3560, loss: 508.69032353907824\n",
      "epoch: 0, step: 3570, loss: 509.84891241043806\n",
      "epoch: 0, step: 3580, loss: 511.5674605742097\n",
      "epoch: 0, step: 3590, loss: 514.8154292926192\n",
      "epoch: 0, step: 3600, loss: 517.849916473031\n",
      "epoch: 0, step: 3610, loss: 519.676528327167\n",
      "epoch: 0, step: 3620, loss: 520.8852517157793\n",
      "epoch: 0, step: 3630, loss: 522.2016955763102\n",
      "epoch: 0, step: 3640, loss: 523.11309132725\n",
      "epoch: 0, step: 3650, loss: 524.7054252102971\n",
      "epoch: 0, step: 3660, loss: 526.3566702082753\n",
      "epoch: 0, step: 3670, loss: 531.4490722194314\n",
      "epoch: 0, step: 3680, loss: 533.4706958979368\n",
      "epoch: 0, step: 3690, loss: 534.4009781777859\n",
      "epoch: 0, step: 3700, loss: 535.455261066556\n",
      "epoch: 0, step: 3710, loss: 536.5471328571439\n",
      "epoch: 0, step: 3720, loss: 537.4172039031982\n",
      "epoch: 0, step: 3730, loss: 538.7176431044936\n",
      "epoch: 0, step: 3740, loss: 539.9180060476065\n",
      "epoch: 0, step: 3750, loss: 541.06248793751\n",
      "epoch: 0, step: 3760, loss: 542.6070912629366\n",
      "epoch: 0, step: 3770, loss: 543.6038311943412\n",
      "epoch: 0, step: 3780, loss: 544.8153738602996\n",
      "epoch: 0, step: 3790, loss: 545.757679566741\n",
      "epoch: 0, step: 3800, loss: 547.1468960419297\n",
      "epoch: 0, step: 3810, loss: 548.4028234630823\n",
      "epoch: 0, step: 3820, loss: 551.2707095891237\n",
      "epoch: 0, step: 3830, loss: 553.4521350115538\n",
      "epoch: 0, step: 3840, loss: 554.889835678041\n",
      "epoch: 0, step: 3850, loss: 556.3044084385037\n",
      "epoch: 0, step: 3860, loss: 557.2158067077398\n",
      "epoch: 0, step: 3870, loss: 558.599360615015\n",
      "epoch: 0, step: 3880, loss: 559.5115560814738\n",
      "epoch: 0, step: 3890, loss: 561.273471981287\n",
      "epoch: 0, step: 3900, loss: 562.8926327228546\n",
      "epoch: 0, step: 3910, loss: 563.7905176356435\n",
      "epoch: 0, step: 3920, loss: 565.4156699180603\n",
      "epoch: 0, step: 3930, loss: 566.4362662136555\n",
      "epoch: 0, step: 3940, loss: 567.4925330430269\n",
      "epoch: 0, step: 3950, loss: 568.5685018226504\n",
      "epoch: 0, step: 3960, loss: 569.5771177262068\n",
      "epoch: 0, step: 3970, loss: 571.2902092263103\n",
      "epoch: 0, step: 3980, loss: 572.5656363219023\n",
      "epoch: 0, step: 3990, loss: 573.7816435322165\n",
      "epoch: 0, step: 4000, loss: 574.9446781203151\n",
      "epoch: 0, step: 4010, loss: 576.0283999145031\n",
      "epoch: 0, step: 4020, loss: 576.9643758386374\n",
      "epoch: 0, step: 4030, loss: 578.326148815453\n",
      "epoch: 0, step: 4040, loss: 579.835631608963\n",
      "epoch: 0, step: 4050, loss: 580.7825438678265\n",
      "epoch: 0, step: 4060, loss: 582.0750753283501\n",
      "epoch: 0, step: 4070, loss: 583.3260335475206\n",
      "epoch: 0, step: 4080, loss: 585.0787538290024\n",
      "epoch: 0, step: 4090, loss: 586.4598168954253\n",
      "epoch: 0, step: 4100, loss: 589.8570171818137\n",
      "epoch: 0, step: 4110, loss: 591.490344054997\n",
      "epoch: 0, step: 4120, loss: 592.8229561671615\n",
      "epoch: 0, step: 4130, loss: 593.9417686462402\n",
      "epoch: 0, step: 4140, loss: 595.5421610102057\n",
      "epoch: 0, step: 4150, loss: 597.1102147549391\n",
      "epoch: 0, step: 4160, loss: 601.1197917610407\n",
      "epoch: 0, step: 4170, loss: 605.2802561074495\n",
      "epoch: 0, step: 4180, loss: 606.7838212773204\n",
      "epoch: 0, step: 4190, loss: 608.0499068051577\n",
      "epoch: 0, step: 4200, loss: 609.0469917953014\n",
      "epoch: 0, step: 4210, loss: 610.4321657121181\n",
      "epoch: 0, step: 4220, loss: 612.0984168946743\n",
      "epoch: 0, step: 4230, loss: 615.5087985396385\n",
      "epoch: 0, step: 4240, loss: 617.2441354170442\n",
      "epoch: 0, step: 4250, loss: 618.9800889492035\n",
      "epoch: 0, step: 4260, loss: 620.6846668347716\n",
      "epoch: 0, step: 4270, loss: 622.3931260555983\n",
      "epoch: 0, step: 4280, loss: 623.5817345678806\n",
      "epoch: 0, step: 4290, loss: 625.1643229573965\n",
      "epoch: 0, step: 4300, loss: 626.2595688477159\n",
      "epoch: 0, step: 4310, loss: 627.2962937876582\n",
      "epoch: 0, step: 4320, loss: 628.5414254441857\n",
      "epoch: 0, step: 4330, loss: 629.7470176666975\n",
      "epoch: 0, step: 4340, loss: 630.7789533063769\n",
      "epoch: 0, step: 4350, loss: 634.6278508454561\n",
      "epoch: 0, step: 4360, loss: 635.8214484378695\n",
      "epoch: 0, step: 4370, loss: 636.9957828596234\n",
      "epoch: 0, step: 4380, loss: 637.9137794226408\n",
      "epoch: 0, step: 4390, loss: 638.7190994173288\n",
      "epoch: 0, step: 4400, loss: 640.2590524107218\n",
      "epoch: 0, step: 4410, loss: 641.2535828500986\n",
      "epoch: 0, step: 4420, loss: 642.987240575254\n",
      "epoch: 0, step: 4430, loss: 644.6175649836659\n",
      "epoch: 0, step: 4440, loss: 646.1200432181358\n",
      "epoch: 0, step: 4450, loss: 647.2241466641426\n",
      "epoch: 0, step: 4460, loss: 648.4924077764153\n",
      "epoch: 0, step: 4470, loss: 649.505238249898\n",
      "epoch: 0, step: 4480, loss: 650.7552876099944\n",
      "epoch: 0, step: 4490, loss: 651.7384068071842\n",
      "epoch: 0, step: 4500, loss: 652.6403187289834\n",
      "epoch: 0, step: 4510, loss: 653.5341298729181\n",
      "epoch: 0, step: 4520, loss: 654.4429702907801\n",
      "epoch: 0, step: 4530, loss: 655.4698152467608\n",
      "epoch: 0, step: 4540, loss: 656.3951486572623\n",
      "epoch: 0, step: 4550, loss: 657.6511225923896\n",
      "epoch: 0, step: 4560, loss: 659.0040000975132\n",
      "epoch: 0, step: 4570, loss: 660.3240671977401\n",
      "epoch: 0, step: 4580, loss: 661.8609943166375\n",
      "epoch: 0, step: 4590, loss: 662.8959270343184\n",
      "epoch: 0, step: 4600, loss: 663.8837655112147\n",
      "epoch: 0, step: 4610, loss: 664.855591326952\n",
      "epoch: 0, step: 4620, loss: 665.9243012294173\n",
      "epoch: 0, step: 4630, loss: 667.0927561223507\n",
      "epoch: 0, step: 4640, loss: 668.1918473690748\n",
      "epoch: 0, step: 4650, loss: 669.233129657805\n",
      "epoch: 0, step: 4660, loss: 670.5270909518003\n",
      "epoch: 0, step: 4670, loss: 671.6296174451709\n",
      "epoch: 0, step: 4680, loss: 673.2897698804736\n",
      "epoch: 0, step: 4690, loss: 674.4127050116658\n",
      "epoch: 0, step: 4700, loss: 676.4161077737808\n",
      "epoch: 0, step: 4710, loss: 679.944617882371\n",
      "epoch: 0, step: 4720, loss: 681.7292259931564\n",
      "epoch: 0, step: 4730, loss: 683.0321149006486\n",
      "epoch: 0, step: 4740, loss: 683.7890173643827\n",
      "epoch: 0, step: 4750, loss: 684.7723510488868\n",
      "epoch: 0, step: 4760, loss: 685.7843663021922\n",
      "epoch: 0, step: 4770, loss: 687.4124557971954\n",
      "epoch: 0, step: 4780, loss: 688.6378870233893\n",
      "epoch: 0, step: 4790, loss: 690.2039024680853\n",
      "epoch: 0, step: 4800, loss: 691.9506362453103\n",
      "epoch: 0, step: 4810, loss: 693.081476457417\n",
      "epoch: 0, step: 4820, loss: 694.4023443087935\n",
      "epoch: 0, step: 4830, loss: 695.7080041319132\n",
      "epoch: 0, step: 4840, loss: 696.7812293991446\n",
      "epoch: 0, step: 4850, loss: 697.7517853602767\n",
      "epoch: 0, step: 4860, loss: 698.837736800313\n",
      "epoch: 0, step: 4870, loss: 699.9872190058231\n",
      "epoch: 0, step: 4880, loss: 701.1932247430086\n",
      "epoch: 0, step: 4890, loss: 702.2789530754089\n",
      "epoch: 0, step: 4900, loss: 703.2929032221437\n",
      "epoch: 0, step: 4910, loss: 704.514776609838\n",
      "epoch: 0, step: 4920, loss: 705.6193206161261\n",
      "epoch: 0, step: 4930, loss: 706.7946880310774\n",
      "epoch: 0, step: 4940, loss: 707.755522236228\n",
      "epoch: 0, step: 4950, loss: 709.1535552591085\n",
      "epoch: 0, step: 4960, loss: 710.3907826989889\n",
      "epoch: 0, step: 4970, loss: 713.089588150382\n",
      "epoch: 0, step: 4980, loss: 715.2941496670246\n",
      "epoch: 0, step: 4990, loss: 716.5755056887865\n",
      "epoch: 0, step: 5000, loss: 718.0387415066361\n",
      "epoch: 0, step: 5010, loss: 718.9843859523535\n",
      "epoch: 0, step: 5020, loss: 720.7808330133557\n",
      "epoch: 0, step: 5030, loss: 723.1744999960065\n",
      "epoch: 0, step: 5040, loss: 724.2868702113628\n",
      "epoch: 0, step: 5050, loss: 725.434976182878\n",
      "epoch: 0, step: 5060, loss: 726.6865136101842\n",
      "epoch: 0, step: 5070, loss: 728.1453180089593\n",
      "epoch: 0, step: 5080, loss: 729.6521840617061\n",
      "epoch: 0, step: 5090, loss: 730.9483822286129\n",
      "epoch: 0, step: 5100, loss: 732.4455121532083\n",
      "epoch: 0, step: 5110, loss: 735.0183621048927\n",
      "epoch: 0, step: 5120, loss: 736.4869072288275\n",
      "epoch: 0, step: 5130, loss: 737.6250712200999\n",
      "epoch: 0, step: 5140, loss: 738.9270480275154\n",
      "epoch: 0, step: 5150, loss: 740.1389044523239\n",
      "epoch: 0, step: 5160, loss: 741.3395625725389\n",
      "epoch: 0, step: 5170, loss: 742.4265462458134\n",
      "epoch: 0, step: 5180, loss: 743.3659665882587\n",
      "epoch: 0, step: 5190, loss: 744.375945173204\n",
      "epoch: 0, step: 5200, loss: 746.0507398471236\n",
      "epoch: 0, step: 5210, loss: 747.4241292178631\n",
      "epoch: 0, step: 5220, loss: 748.3162895664573\n",
      "epoch: 0, step: 5230, loss: 749.5735714063048\n",
      "epoch: 0, step: 5240, loss: 750.4959093108773\n",
      "epoch: 0, step: 5250, loss: 751.7510478422046\n",
      "epoch: 0, step: 5260, loss: 753.3000913709402\n",
      "epoch: 0, step: 5270, loss: 755.3572224527597\n",
      "epoch: 0, step: 5280, loss: 756.6097520589828\n",
      "epoch: 0, step: 5290, loss: 757.6949197426438\n",
      "epoch: 0, step: 5300, loss: 759.0628720372915\n",
      "epoch: 0, step: 5310, loss: 761.5304716974497\n",
      "epoch: 0, step: 5320, loss: 763.2178236320615\n",
      "epoch: 0, step: 5330, loss: 764.5030917003751\n",
      "epoch: 0, step: 5340, loss: 765.8596192523837\n",
      "epoch: 0, step: 5350, loss: 766.9619503542781\n",
      "epoch: 0, step: 5360, loss: 767.873754195869\n",
      "epoch: 0, step: 5370, loss: 769.2280153185129\n",
      "epoch: 0, step: 5380, loss: 770.0915117636323\n",
      "epoch: 0, step: 5390, loss: 770.9956090599298\n",
      "epoch: 0, step: 5400, loss: 772.0122028812766\n",
      "epoch: 0, step: 5410, loss: 773.2017194777727\n",
      "epoch: 0, step: 5420, loss: 774.3146364316344\n",
      "epoch: 0, step: 5430, loss: 775.4192297533154\n",
      "epoch: 0, step: 5440, loss: 776.1874726563692\n",
      "epoch: 0, step: 5450, loss: 777.336594901979\n",
      "epoch: 0, step: 5460, loss: 778.3851488381624\n",
      "epoch: 0, step: 5470, loss: 779.526053018868\n",
      "epoch: 0, step: 5480, loss: 780.7050081267953\n",
      "epoch: 0, step: 5490, loss: 782.8408145457506\n",
      "epoch: 0, step: 5500, loss: 784.358026355505\n",
      "epoch: 0, step: 5510, loss: 785.5674580484629\n",
      "epoch: 0, step: 5520, loss: 786.8126200437546\n",
      "epoch: 0, step: 5530, loss: 787.87453699857\n",
      "epoch: 0, step: 5540, loss: 789.0951738581061\n",
      "epoch: 0, step: 5550, loss: 790.0440444722772\n",
      "epoch: 0, step: 5560, loss: 791.1613404229283\n",
      "epoch: 0, step: 5570, loss: 792.3354476466775\n",
      "epoch: 0, step: 5580, loss: 793.4175315871835\n",
      "epoch: 0, step: 5590, loss: 794.312213614583\n",
      "epoch: 0, step: 5600, loss: 795.6706927791238\n",
      "epoch: 0, step: 5610, loss: 796.8859242722392\n",
      "epoch: 0, step: 5620, loss: 798.3459687009454\n",
      "epoch: 0, step: 5630, loss: 799.8636103793979\n",
      "epoch: 0, step: 5640, loss: 801.1173489913344\n",
      "epoch: 0, step: 5650, loss: 801.9960797354579\n",
      "epoch: 0, step: 5660, loss: 802.8081791624427\n",
      "epoch: 0, step: 5670, loss: 803.9257729426026\n",
      "epoch: 0, step: 5680, loss: 804.8556838929653\n",
      "epoch: 0, step: 5690, loss: 805.9988174214959\n",
      "epoch: 0, step: 5700, loss: 806.9614000618458\n",
      "epoch: 0, step: 5710, loss: 808.3229872658849\n",
      "epoch: 0, step: 5720, loss: 811.2605328336358\n",
      "epoch: 0, step: 5730, loss: 812.4395813494921\n",
      "epoch: 0, step: 5740, loss: 813.4769133627415\n",
      "epoch: 0, step: 5750, loss: 814.5092225149274\n",
      "epoch: 0, step: 5760, loss: 815.6376696154475\n",
      "epoch: 0, step: 5770, loss: 817.1096535399556\n",
      "epoch: 0, step: 5780, loss: 818.1338578164577\n",
      "epoch: 0, step: 5790, loss: 819.2535363137722\n",
      "epoch: 0, step: 5800, loss: 820.4286870658398\n",
      "epoch: 0, step: 5810, loss: 823.7172853350639\n",
      "epoch: 0, step: 5820, loss: 826.6292570829391\n",
      "epoch: 0, step: 5830, loss: 827.8265744224191\n",
      "epoch: 0, step: 5840, loss: 829.4454179182649\n",
      "epoch: 0, step: 5850, loss: 830.8340183421969\n",
      "epoch: 0, step: 5860, loss: 831.7078235149384\n",
      "epoch: 0, step: 5870, loss: 832.8124807700515\n",
      "epoch: 0, step: 5880, loss: 834.1989883482456\n",
      "epoch: 0, step: 5890, loss: 835.363604195416\n",
      "epoch: 0, step: 5900, loss: 836.3281623870134\n",
      "epoch: 0, step: 5910, loss: 837.2440605163574\n",
      "epoch: 0, step: 5920, loss: 838.2545305863023\n",
      "epoch: 0, step: 5930, loss: 839.2789938822389\n",
      "epoch: 0, step: 5940, loss: 840.5864994227886\n",
      "epoch: 0, step: 5950, loss: 841.6001876145601\n",
      "epoch: 0, step: 5960, loss: 842.6201505362988\n",
      "epoch: 0, step: 5970, loss: 843.6472360193729\n",
      "epoch: 0, step: 5980, loss: 844.6325454562902\n",
      "epoch: 0, step: 5990, loss: 845.7012947797775\n",
      "epoch: 0, step: 6000, loss: 846.8427877649665\n",
      "epoch: 0, step: 6010, loss: 847.9485900327563\n",
      "epoch: 0, step: 6020, loss: 849.0770524889231\n",
      "epoch: 0, step: 6030, loss: 850.0587744936347\n",
      "epoch: 0, step: 6040, loss: 850.951860845089\n",
      "epoch: 0, step: 6050, loss: 851.904997907579\n",
      "epoch: 0, step: 6060, loss: 853.0102608576417\n",
      "epoch: 0, step: 6070, loss: 854.1130782961845\n",
      "epoch: 0, step: 6080, loss: 855.2960143834352\n",
      "epoch: 0, step: 6090, loss: 856.4242838993669\n",
      "epoch: 0, step: 6100, loss: 857.4877464845777\n",
      "epoch: 0, step: 6110, loss: 858.443136960268\n",
      "epoch: 0, step: 6120, loss: 860.5037680119276\n",
      "epoch: 0, step: 6130, loss: 861.718222334981\n",
      "epoch: 0, step: 6140, loss: 862.6225753799081\n",
      "epoch: 0, step: 6150, loss: 863.6900018304586\n",
      "epoch: 0, step: 6160, loss: 864.9272714480758\n",
      "epoch: 0, step: 6170, loss: 866.213867738843\n",
      "epoch: 0, step: 6180, loss: 867.3167667910457\n",
      "epoch: 0, step: 6190, loss: 869.0688277259469\n",
      "epoch: 0, step: 6200, loss: 870.2137162461877\n",
      "epoch: 0, step: 6210, loss: 871.4028657078743\n",
      "epoch: 0, step: 6220, loss: 872.7104248031974\n",
      "epoch: 0, step: 6230, loss: 873.6571251004934\n",
      "epoch: 0, step: 6240, loss: 874.7547566369176\n",
      "epoch: 0, step: 6250, loss: 875.6357092037797\n",
      "epoch: 0, step: 6260, loss: 876.489750534296\n",
      "epoch: 0, step: 6270, loss: 877.3425094336271\n",
      "epoch: 0, step: 6280, loss: 878.2301034480333\n",
      "epoch: 0, step: 6290, loss: 879.4591814950109\n",
      "epoch: 0, step: 6300, loss: 880.5458342134953\n",
      "epoch: 0, step: 6310, loss: 881.948427438736\n",
      "epoch: 0, step: 6320, loss: 883.9863240718842\n",
      "epoch: 0, step: 6330, loss: 885.7429051101208\n",
      "epoch: 0, step: 6340, loss: 888.9707825481892\n",
      "epoch: 0, step: 6350, loss: 892.3349093794823\n",
      "epoch: 0, step: 6360, loss: 895.4611400663853\n",
      "epoch: 0, step: 6370, loss: 896.5351127833128\n",
      "epoch: 0, step: 6380, loss: 897.5036333650351\n",
      "epoch: 0, step: 6390, loss: 898.5892425477505\n",
      "epoch: 0, step: 6400, loss: 899.7878310754895\n",
      "epoch: 0, step: 6410, loss: 900.9203635379672\n",
      "epoch: 0, step: 6420, loss: 901.8101619258523\n",
      "epoch: 0, step: 6430, loss: 903.0999332368374\n",
      "epoch: 0, step: 6440, loss: 904.2563015967607\n",
      "epoch: 0, step: 6450, loss: 905.1075694933534\n",
      "epoch: 0, step: 6460, loss: 906.1430260315537\n",
      "epoch: 0, step: 6470, loss: 908.1458927616477\n",
      "epoch: 0, step: 6480, loss: 911.4362317845225\n",
      "epoch: 0, step: 6490, loss: 914.7528979256749\n",
      "epoch: 0, step: 6500, loss: 918.5693440809846\n",
      "epoch: 0, step: 6510, loss: 920.0301294252276\n",
      "epoch: 0, step: 6520, loss: 921.2833002358675\n",
      "epoch: 0, step: 6530, loss: 923.1888130828738\n",
      "epoch: 0, step: 6540, loss: 924.2073476910591\n",
      "epoch: 0, step: 6550, loss: 925.3595029115677\n",
      "epoch: 0, step: 6560, loss: 926.9995252117515\n",
      "epoch: 0, step: 6570, loss: 928.1205473765731\n",
      "epoch: 0, step: 6580, loss: 929.2823565229774\n",
      "epoch: 0, step: 6590, loss: 930.1893738135695\n",
      "epoch: 0, step: 6600, loss: 931.3073322847486\n",
      "epoch: 0, step: 6610, loss: 932.5262502729893\n",
      "epoch: 0, step: 6620, loss: 934.7286195829511\n",
      "epoch: 0, step: 6630, loss: 936.1201994642615\n",
      "epoch: 0, step: 6640, loss: 937.2327339202166\n",
      "epoch: 0, step: 6650, loss: 938.4630690440536\n",
      "epoch: 0, step: 6660, loss: 939.5645046308637\n",
      "epoch: 0, step: 6670, loss: 940.4498434439301\n",
      "epoch: 0, step: 6680, loss: 941.4430058151484\n",
      "epoch: 0, step: 6690, loss: 942.5042967423797\n",
      "epoch: 0, step: 6700, loss: 943.446444272995\n",
      "epoch: 0, step: 6710, loss: 944.5576359182596\n",
      "epoch: 0, step: 6720, loss: 945.9354891479015\n",
      "epoch: 0, step: 6730, loss: 946.7830646038055\n",
      "epoch: 0, step: 6740, loss: 947.8040743023157\n",
      "epoch: 0, step: 6750, loss: 949.2350836321712\n",
      "epoch: 0, step: 6760, loss: 950.5844544544816\n",
      "epoch: 0, step: 6770, loss: 951.6522840410471\n",
      "epoch: 0, step: 6780, loss: 952.5502715185285\n",
      "epoch: 0, step: 6790, loss: 953.5093166083097\n",
      "epoch: 0, step: 6800, loss: 954.3548247739673\n",
      "epoch: 0, step: 6810, loss: 955.263879403472\n",
      "epoch: 0, step: 6820, loss: 956.4231555834413\n",
      "epoch: 0, step: 6830, loss: 957.3296451494098\n",
      "epoch: 0, step: 6840, loss: 958.3975992575288\n",
      "epoch: 0, step: 6850, loss: 959.3287961557508\n",
      "epoch: 0, step: 6860, loss: 960.9657935649157\n",
      "epoch: 0, step: 6870, loss: 961.9676861688495\n",
      "epoch: 0, step: 6880, loss: 963.7247460708022\n",
      "epoch: 0, step: 6890, loss: 966.4253904297948\n",
      "epoch: 0, step: 6900, loss: 968.4578644186258\n",
      "epoch: 0, step: 6910, loss: 969.5331186279655\n",
      "epoch: 0, step: 6920, loss: 970.7063447609544\n",
      "epoch: 0, step: 6930, loss: 971.5595097318292\n",
      "epoch: 0, step: 6940, loss: 972.6060655117035\n",
      "epoch: 0, step: 6950, loss: 975.0822707638144\n",
      "epoch: 0, step: 6960, loss: 979.1335392221808\n",
      "epoch: 0, step: 6970, loss: 982.8531689122319\n",
      "epoch: 0, step: 6980, loss: 986.9009147658944\n",
      "epoch: 0, step: 6990, loss: 989.0428323224187\n",
      "epoch: 0, step: 7000, loss: 990.2727326452732\n",
      "epoch: 0, step: 7010, loss: 991.512005507946\n",
      "epoch: 0, step: 7020, loss: 992.9407721608877\n",
      "epoch: 0, step: 7030, loss: 994.1619416326284\n",
      "epoch: 0, step: 7040, loss: 995.2327413260937\n",
      "epoch: 0, step: 7050, loss: 996.6092658936977\n",
      "epoch: 0, step: 7060, loss: 997.8653278872371\n",
      "epoch: 0, step: 7070, loss: 999.1447676047683\n",
      "epoch: 0, step: 7080, loss: 1000.2626231536269\n",
      "epoch: 0, step: 7090, loss: 1001.4204477593303\n",
      "epoch: 0, step: 7100, loss: 1002.5013888329268\n",
      "epoch: 0, step: 7110, loss: 1003.4472772926092\n",
      "epoch: 0, step: 7120, loss: 1004.4127752557397\n",
      "epoch: 0, step: 7130, loss: 1005.5033167377114\n",
      "epoch: 0, step: 7140, loss: 1006.8401163965464\n",
      "epoch: 0, step: 7150, loss: 1008.2435439229012\n",
      "epoch: 0, step: 7160, loss: 1009.4088430330157\n",
      "epoch: 0, step: 7170, loss: 1010.9031653553247\n",
      "epoch: 0, step: 7180, loss: 1012.2637701109052\n",
      "epoch: 0, step: 7190, loss: 1013.1274154484272\n",
      "epoch: 0, step: 7200, loss: 1014.197832159698\n",
      "epoch: 0, step: 7210, loss: 1015.9968343302608\n",
      "epoch: 0, step: 7220, loss: 1017.4101304486394\n",
      "epoch: 0, step: 7230, loss: 1018.443623393774\n",
      "epoch: 0, step: 7240, loss: 1019.7027725577354\n",
      "epoch: 0, step: 7250, loss: 1020.7635586857796\n",
      "epoch: 0, step: 7260, loss: 1022.0346222743392\n",
      "epoch: 0, step: 7270, loss: 1023.2077867388725\n",
      "epoch: 0, step: 7280, loss: 1024.2702766731381\n",
      "epoch: 0, step: 7290, loss: 1025.4932536631823\n",
      "epoch: 0, step: 7300, loss: 1026.8310001194477\n",
      "epoch: 0, step: 7310, loss: 1028.1824475824833\n",
      "epoch: 0, step: 7320, loss: 1029.2563650235534\n",
      "epoch: 0, step: 7330, loss: 1030.2941042929888\n",
      "epoch: 0, step: 7340, loss: 1031.3409845232964\n",
      "epoch: 0, step: 7350, loss: 1032.4618376940489\n",
      "epoch: 0, step: 7360, loss: 1033.48674236238\n",
      "epoch: 0, step: 7370, loss: 1034.4350331127644\n",
      "epoch: 0, step: 7380, loss: 1035.2796615734696\n",
      "epoch: 0, step: 7390, loss: 1036.461053557694\n",
      "epoch: 0, step: 7400, loss: 1037.8191159442067\n",
      "epoch: 0, step: 7410, loss: 1038.782101251185\n",
      "epoch: 0, step: 7420, loss: 1040.1656459718943\n",
      "epoch: 0, step: 7430, loss: 1041.1527840048075\n",
      "epoch: 0, step: 7440, loss: 1043.2597630098462\n",
      "epoch: 0, step: 7450, loss: 1044.3692544698715\n",
      "epoch: 0, step: 7460, loss: 1045.2441282719374\n",
      "epoch: 0, step: 7470, loss: 1046.0580440163612\n",
      "epoch: 0, step: 7480, loss: 1047.0915957391262\n",
      "epoch: 0, step: 7490, loss: 1048.2211203724146\n",
      "epoch: 0, step: 7500, loss: 1049.1611262261868\n",
      "epoch: 0, step: 7510, loss: 1050.2320081442595\n",
      "epoch: 0, step: 7520, loss: 1051.2621470838785\n",
      "epoch: 0, step: 7530, loss: 1052.9621740952134\n",
      "epoch: 0, step: 7540, loss: 1055.3171251118183\n",
      "epoch: 0, step: 7550, loss: 1057.802917599678\n",
      "epoch: 0, step: 7560, loss: 1059.3958392217755\n",
      "epoch: 0, step: 7570, loss: 1060.4024803116918\n",
      "epoch: 0, step: 7580, loss: 1061.7857770100236\n",
      "epoch: 0, step: 7590, loss: 1063.299318768084\n",
      "epoch: 0, step: 7600, loss: 1064.3408870249987\n",
      "epoch: 0, step: 7610, loss: 1066.3528805151582\n",
      "epoch: 0, step: 7620, loss: 1067.2768471315503\n",
      "epoch: 0, step: 7630, loss: 1068.4777043536305\n",
      "epoch: 0, step: 7640, loss: 1070.1512297838926\n",
      "epoch: 0, step: 7650, loss: 1071.1880254969\n",
      "epoch: 0, step: 7660, loss: 1072.2258779257536\n",
      "epoch: 0, step: 7670, loss: 1073.177551716566\n",
      "epoch: 0, step: 7680, loss: 1074.3532544001937\n",
      "epoch: 0, step: 7690, loss: 1075.5788553878665\n",
      "epoch: 0, step: 7700, loss: 1076.7228933274746\n",
      "epoch: 0, step: 7710, loss: 1077.8762834221125\n",
      "epoch: 0, step: 7720, loss: 1079.1120055913925\n",
      "epoch: 0, step: 7730, loss: 1080.2926636561751\n",
      "epoch: 0, step: 7740, loss: 1081.3471619188786\n",
      "epoch: 0, step: 7750, loss: 1082.4782271534204\n",
      "epoch: 0, step: 7760, loss: 1084.0487633123994\n",
      "epoch: 0, step: 7770, loss: 1084.916124753654\n",
      "epoch: 0, step: 7780, loss: 1085.8393109366298\n",
      "epoch: 0, step: 7790, loss: 1086.8049665391445\n",
      "epoch: 0, step: 7800, loss: 1087.9662074297667\n",
      "epoch: 0, step: 7810, loss: 1088.909673564136\n",
      "epoch: 0, step: 7820, loss: 1090.0298923626542\n",
      "epoch: 0, step: 7830, loss: 1090.8732229620218\n",
      "epoch: 0, step: 7840, loss: 1091.8943835869431\n",
      "epoch: 0, step: 7850, loss: 1092.8126756176353\n",
      "epoch: 0, step: 7860, loss: 1093.8142022788525\n",
      "epoch: 0, step: 7870, loss: 1095.3491912186146\n",
      "epoch: 0, step: 7880, loss: 1096.4952971711755\n",
      "epoch: 0, step: 7890, loss: 1097.3040172979236\n",
      "epoch: 0, step: 7900, loss: 1098.0198386386037\n",
      "epoch: 0, step: 7910, loss: 1098.7671599537134\n",
      "epoch: 0, step: 7920, loss: 1099.7413127496839\n",
      "epoch: 0, step: 7930, loss: 1100.7143823429942\n",
      "epoch: 0, step: 7940, loss: 1101.8987442329526\n",
      "epoch: 0, step: 7950, loss: 1103.3907131031156\n",
      "epoch: 0, step: 7960, loss: 1104.6154676005244\n",
      "epoch: 0, step: 7970, loss: 1105.5783567503095\n",
      "epoch: 0, step: 7980, loss: 1106.9930085390806\n",
      "epoch: 0, step: 7990, loss: 1107.9450347870588\n",
      "epoch: 0, step: 8000, loss: 1109.761963389814\n",
      "epoch: 0, step: 8010, loss: 1110.8744037896395\n",
      "epoch: 0, step: 8020, loss: 1112.0352601259947\n",
      "epoch: 0, step: 8030, loss: 1112.9401054382324\n",
      "epoch: 0, step: 8040, loss: 1114.0485002696514\n",
      "epoch: 0, step: 8050, loss: 1115.2394380569458\n",
      "epoch: 0, step: 8060, loss: 1116.3337545096874\n",
      "epoch: 0, step: 8070, loss: 1117.1656877696514\n",
      "epoch: 0, step: 8080, loss: 1118.074277818203\n",
      "epoch: 0, step: 8090, loss: 1119.2953522205353\n",
      "epoch: 0, step: 8100, loss: 1120.4641989991069\n",
      "epoch: 0, step: 8110, loss: 1121.6127814278007\n",
      "epoch: 0, step: 8120, loss: 1124.2856777235866\n",
      "epoch: 0, step: 8130, loss: 1125.5307753011584\n",
      "epoch: 0, step: 8140, loss: 1126.385095320642\n",
      "epoch: 0, step: 8150, loss: 1127.2034957632422\n",
      "epoch: 0, step: 8160, loss: 1128.260824173689\n",
      "epoch: 0, step: 8170, loss: 1129.9590554386377\n",
      "epoch: 0, step: 8180, loss: 1130.801787726581\n",
      "epoch: 0, step: 8190, loss: 1131.6408768072724\n",
      "epoch: 0, step: 8200, loss: 1132.8453744724393\n",
      "epoch: 0, step: 8210, loss: 1133.8705029785633\n",
      "epoch: 0, step: 8220, loss: 1135.0208180174232\n",
      "epoch: 0, step: 8230, loss: 1136.2508382424712\n",
      "epoch: 0, step: 8240, loss: 1137.6957495212555\n",
      "epoch: 0, step: 8250, loss: 1139.0394058972597\n",
      "epoch: 0, step: 8260, loss: 1140.0186255723238\n",
      "epoch: 0, step: 8270, loss: 1141.2394546493888\n",
      "epoch: 0, step: 8280, loss: 1142.0845903381705\n",
      "epoch: 0, step: 8290, loss: 1144.3571976274252\n",
      "epoch: 0, step: 8300, loss: 1145.253409191966\n",
      "epoch: 0, step: 8310, loss: 1146.4880711734295\n",
      "epoch: 0, step: 8320, loss: 1147.7623041570187\n",
      "epoch: 0, step: 8330, loss: 1148.7940866872668\n",
      "epoch: 0, step: 8340, loss: 1150.6629609391093\n",
      "epoch: 0, step: 8350, loss: 1151.9529494792223\n",
      "epoch: 0, step: 8360, loss: 1152.9007244333625\n",
      "epoch: 0, step: 8370, loss: 1154.5345124229789\n",
      "epoch: 0, step: 8380, loss: 1155.5833430364728\n",
      "epoch: 0, step: 8390, loss: 1156.6224960535765\n",
      "epoch: 0, step: 8400, loss: 1157.61628562212\n",
      "epoch: 0, step: 8410, loss: 1158.6267992407084\n",
      "epoch: 0, step: 8420, loss: 1159.9051692262292\n",
      "epoch: 0, step: 8430, loss: 1161.2382070049644\n",
      "epoch: 0, step: 8440, loss: 1162.3932416960597\n",
      "epoch: 0, step: 8450, loss: 1163.4646532833576\n",
      "epoch: 0, step: 8460, loss: 1164.3100648596883\n",
      "epoch: 0, step: 8470, loss: 1165.427600108087\n",
      "epoch: 0, step: 8480, loss: 1166.4156872555614\n",
      "epoch: 0, step: 8490, loss: 1168.1393077000976\n",
      "epoch: 0, step: 8500, loss: 1169.3086070120335\n",
      "epoch: 0, step: 8510, loss: 1170.2179996296763\n",
      "epoch: 0, step: 8520, loss: 1171.3374685421586\n",
      "epoch: 0, step: 8530, loss: 1172.2998302951455\n",
      "epoch: 0, step: 8540, loss: 1174.3325035348535\n",
      "epoch: 0, step: 8550, loss: 1175.4919079467654\n",
      "epoch: 0, step: 8560, loss: 1178.0058965459466\n",
      "epoch: 0, step: 8570, loss: 1181.0904554203153\n",
      "epoch: 0, step: 8580, loss: 1183.9040477573872\n",
      "epoch: 0, step: 8590, loss: 1185.0519023388624\n",
      "epoch: 0, step: 8600, loss: 1186.0723371878266\n",
      "epoch: 0, step: 8610, loss: 1187.0545305907726\n",
      "epoch: 0, step: 8620, loss: 1188.0504707098007\n",
      "epoch: 0, step: 8630, loss: 1189.2003631144762\n",
      "epoch: 0, step: 8640, loss: 1190.2940604761243\n",
      "epoch: 0, step: 8650, loss: 1191.2660488337278\n",
      "epoch: 0, step: 8660, loss: 1192.2416140064597\n",
      "epoch: 0, step: 8670, loss: 1193.2432760745287\n",
      "epoch: 0, step: 8680, loss: 1194.492591008544\n",
      "epoch: 0, step: 8690, loss: 1195.7177029028535\n",
      "epoch: 0, step: 8700, loss: 1197.480171509087\n",
      "epoch: 0, step: 8710, loss: 1199.147783830762\n",
      "epoch: 0, step: 8720, loss: 1200.3450847491622\n",
      "epoch: 0, step: 8730, loss: 1201.6289766952395\n",
      "epoch: 0, step: 8740, loss: 1202.6013806015253\n",
      "epoch: 0, step: 8750, loss: 1203.3844178840518\n",
      "epoch: 0, step: 8760, loss: 1207.0115133151412\n",
      "epoch: 0, step: 8770, loss: 1209.0985699668527\n",
      "epoch: 0, step: 8780, loss: 1213.5541994571686\n",
      "epoch: 0, step: 8790, loss: 1217.3731948137283\n",
      "epoch: 0, step: 8800, loss: 1221.0542712062597\n",
      "epoch: 0, step: 8810, loss: 1224.4841909855604\n",
      "epoch: 0, step: 8820, loss: 1226.9404259249568\n",
      "epoch: 0, step: 8830, loss: 1230.2078175917268\n",
      "epoch: 0, step: 8840, loss: 1233.0238920077682\n",
      "epoch: 0, step: 8850, loss: 1234.0280007869005\n",
      "epoch: 0, step: 8860, loss: 1235.0058839917183\n",
      "epoch: 0, step: 8870, loss: 1236.2388476729393\n",
      "epoch: 0, step: 8880, loss: 1237.5514700636268\n",
      "epoch: 0, step: 8890, loss: 1238.5763315334916\n",
      "epoch: 0, step: 8900, loss: 1239.5472615361214\n",
      "epoch: 0, step: 8910, loss: 1240.8223814666271\n",
      "epoch: 0, step: 8920, loss: 1242.051205061376\n",
      "epoch: 0, step: 8930, loss: 1243.5023059993982\n",
      "epoch: 0, step: 8940, loss: 1244.4341715797782\n",
      "epoch: 0, step: 8950, loss: 1246.6253174766898\n",
      "epoch: 0, step: 8960, loss: 1247.701394893229\n",
      "epoch: 0, step: 8970, loss: 1248.9744333326817\n",
      "epoch: 0, step: 8980, loss: 1250.2816254198551\n",
      "epoch: 0, step: 8990, loss: 1251.4570529088378\n",
      "epoch: 0, step: 9000, loss: 1252.5212084054947\n",
      "epoch: 0, step: 9010, loss: 1253.5719688758254\n",
      "epoch: 0, step: 9020, loss: 1254.7705480754375\n",
      "epoch: 0, step: 9030, loss: 1255.7110530138016\n",
      "epoch: 0, step: 9040, loss: 1257.0726591199636\n",
      "epoch: 0, step: 9050, loss: 1258.3138089850545\n",
      "epoch: 0, step: 9060, loss: 1259.6647080406547\n",
      "epoch: 0, step: 9070, loss: 1260.5589946955442\n",
      "epoch: 0, step: 9080, loss: 1261.545620188117\n",
      "epoch: 0, step: 9090, loss: 1262.5090031176805\n",
      "epoch: 0, step: 9100, loss: 1264.132441148162\n",
      "epoch: 0, step: 9110, loss: 1267.1350728869438\n",
      "epoch: 0, step: 9120, loss: 1268.1472711190581\n",
      "epoch: 0, step: 9130, loss: 1269.2333510443568\n",
      "epoch: 0, step: 9140, loss: 1270.7750106155872\n",
      "epoch: 0, step: 9150, loss: 1272.0159720480442\n",
      "epoch: 0, step: 9160, loss: 1273.1419193148613\n",
      "epoch: 0, step: 9170, loss: 1274.5930413082242\n",
      "epoch: 0, step: 9180, loss: 1275.6478929445148\n",
      "epoch: 0, step: 9190, loss: 1276.7424538731575\n",
      "epoch: 0, step: 9200, loss: 1278.2842260450125\n",
      "epoch: 0, step: 9210, loss: 1280.393489792943\n",
      "epoch: 0, step: 9220, loss: 1282.9417769610882\n",
      "epoch: 0, step: 9230, loss: 1284.8153047561646\n",
      "epoch: 0, step: 9240, loss: 1286.5382460355759\n",
      "epoch: 0, step: 9250, loss: 1288.0642323344946\n",
      "epoch: 0, step: 9260, loss: 1289.16576320678\n",
      "epoch: 0, step: 9270, loss: 1290.2627204433084\n",
      "epoch: 0, step: 9280, loss: 1291.3779866918921\n",
      "epoch: 0, step: 9290, loss: 1292.315004490316\n",
      "epoch: 0, step: 9300, loss: 1293.277625784278\n",
      "epoch: 0, step: 9310, loss: 1294.1405495628715\n",
      "epoch: 0, step: 9320, loss: 1295.1060909330845\n",
      "epoch: 0, step: 9330, loss: 1296.1148904263973\n",
      "epoch: 0, step: 9340, loss: 1297.0602461025119\n",
      "epoch: 0, step: 9350, loss: 1298.212039731443\n",
      "epoch: 0, step: 9360, loss: 1299.1496030539274\n",
      "epoch: 0, step: 9370, loss: 1300.0018429681659\n",
      "epoch: 0, step: 9380, loss: 1301.395539186895\n",
      "epoch: 0, step: 9390, loss: 1303.2219030559063\n",
      "epoch: 0, step: 9400, loss: 1305.1953896582127\n",
      "epoch: 0, step: 9410, loss: 1306.3721078187227\n",
      "epoch: 0, step: 9420, loss: 1307.428477436304\n",
      "epoch: 0, step: 9430, loss: 1308.6649616882205\n",
      "epoch: 0, step: 9440, loss: 1309.9510960280895\n",
      "epoch: 0, step: 9450, loss: 1310.9024198278785\n",
      "epoch: 0, step: 9460, loss: 1312.1250892058015\n",
      "epoch: 0, step: 9470, loss: 1313.1542458161712\n",
      "epoch: 0, step: 9480, loss: 1314.4371907487512\n",
      "epoch: 0, step: 9490, loss: 1315.3961810395122\n",
      "epoch: 0, step: 9500, loss: 1316.3766164779663\n",
      "epoch: 0, step: 9510, loss: 1317.3842880129814\n",
      "epoch: 0, step: 9520, loss: 1318.6635504290462\n",
      "epoch: 0, step: 9530, loss: 1321.2963097169995\n",
      "epoch: 0, step: 9540, loss: 1322.6308273747563\n",
      "epoch: 0, step: 9550, loss: 1323.8328997120261\n",
      "epoch: 0, step: 9560, loss: 1325.0660506859422\n",
      "epoch: 0, step: 9570, loss: 1326.6625973284245\n",
      "epoch: 0, step: 9580, loss: 1327.8280837610364\n",
      "epoch: 0, step: 9590, loss: 1328.6506231427193\n",
      "epoch: 0, step: 9600, loss: 1329.6816834509373\n",
      "epoch: 0, step: 9610, loss: 1331.4432104974985\n",
      "epoch: 0, step: 9620, loss: 1332.4683734327555\n",
      "epoch: 0, step: 9630, loss: 1333.6677257791162\n",
      "epoch: 0, step: 9640, loss: 1335.1776870116591\n",
      "epoch: 0, step: 9650, loss: 1336.6705389544368\n",
      "epoch: 0, step: 9660, loss: 1339.9522161707282\n",
      "epoch: 0, step: 9670, loss: 1341.8849612176418\n",
      "epoch: 0, step: 9680, loss: 1343.9685948714614\n",
      "epoch: 0, step: 9690, loss: 1347.0661555752158\n",
      "epoch: 0, step: 9700, loss: 1348.8289321511984\n",
      "epoch: 0, step: 9710, loss: 1349.7870398014784\n",
      "epoch: 0, step: 9720, loss: 1350.9095730185509\n",
      "epoch: 0, step: 9730, loss: 1352.0281237587333\n",
      "epoch: 0, step: 9740, loss: 1353.3921286687255\n",
      "epoch: 0, step: 9750, loss: 1355.021078698337\n",
      "epoch: 0, step: 9760, loss: 1356.4139376208186\n",
      "epoch: 0, step: 9770, loss: 1357.6436595022678\n",
      "epoch: 0, step: 9780, loss: 1358.5727476924658\n",
      "epoch: 0, step: 9790, loss: 1361.363673478365\n",
      "epoch: 0, step: 9800, loss: 1362.4731475934386\n",
      "epoch: 0, step: 9810, loss: 1363.3774309530854\n",
      "epoch: 0, step: 9820, loss: 1364.6463162824512\n",
      "epoch: 0, step: 9830, loss: 1365.7785516679287\n",
      "epoch: 0, step: 9840, loss: 1366.6675080284476\n",
      "epoch: 0, step: 9850, loss: 1368.031918913126\n",
      "epoch: 0, step: 9860, loss: 1369.6644798591733\n",
      "epoch: 0, step: 9870, loss: 1370.6490651145577\n",
      "epoch: 0, step: 9880, loss: 1372.4667205959558\n",
      "epoch: 0, step: 9890, loss: 1373.7591731473804\n",
      "epoch: 0, step: 9900, loss: 1374.7482194453478\n",
      "epoch: 0, step: 9910, loss: 1375.6656572446227\n",
      "epoch: 0, step: 9920, loss: 1376.7219748720527\n",
      "epoch: 0, step: 9930, loss: 1378.1645859330893\n",
      "epoch: 0, step: 9940, loss: 1379.100955747068\n",
      "epoch: 0, step: 9950, loss: 1380.2484200894833\n",
      "epoch: 0, step: 9960, loss: 1381.2265847027302\n",
      "epoch: 0, step: 9970, loss: 1382.3090438619256\n",
      "epoch: 0, step: 9980, loss: 1383.687972009182\n",
      "epoch: 0, step: 9990, loss: 1384.6791794747114\n",
      "epoch: 0, step: 10000, loss: 1385.5865452885628\n",
      "epoch: 0, step: 10010, loss: 1386.71654907614\n",
      "epoch: 0, step: 10020, loss: 1387.589176632464\n",
      "epoch: 0, step: 10030, loss: 1388.4653169959784\n",
      "epoch: 0, step: 10040, loss: 1389.5958626940846\n",
      "epoch: 0, step: 10050, loss: 1390.5208424702287\n",
      "epoch: 0, step: 10060, loss: 1391.9256913810968\n",
      "epoch: 0, step: 10070, loss: 1393.3074790164828\n",
      "epoch: 0, step: 10080, loss: 1394.305113054812\n",
      "epoch: 0, step: 10090, loss: 1395.1583582535386\n",
      "epoch: 0, step: 10100, loss: 1396.4976179301739\n",
      "epoch: 0, step: 10110, loss: 1397.455548852682\n",
      "epoch: 0, step: 10120, loss: 1398.3291052281857\n",
      "epoch: 0, step: 10130, loss: 1399.5931243598461\n",
      "epoch: 0, step: 10140, loss: 1400.664471283555\n",
      "epoch: 0, step: 10150, loss: 1401.4617765918374\n",
      "epoch: 0, step: 10160, loss: 1402.2626653909683\n",
      "epoch: 0, step: 10170, loss: 1403.3980440422893\n",
      "epoch: 0, step: 10180, loss: 1404.3703714162111\n",
      "epoch: 0, step: 10190, loss: 1405.1140404567122\n",
      "epoch: 0, step: 10200, loss: 1406.0235965549946\n",
      "epoch: 0, step: 10210, loss: 1407.1210729852319\n",
      "epoch: 0, step: 10220, loss: 1408.1994674801826\n",
      "epoch: 0, step: 10230, loss: 1409.2847016453743\n",
      "epoch: 0, step: 10240, loss: 1410.463833667338\n",
      "epoch: 0, step: 10250, loss: 1411.8036621958017\n",
      "epoch: 0, step: 10260, loss: 1412.8609141856432\n",
      "epoch: 0, step: 10270, loss: 1413.7482442259789\n",
      "epoch: 0, step: 10280, loss: 1414.976641267538\n",
      "epoch: 0, step: 10290, loss: 1416.2124743387103\n",
      "epoch: 0, step: 10300, loss: 1417.3636644780636\n",
      "epoch: 0, step: 10310, loss: 1418.9333487823606\n",
      "epoch: 0, step: 10320, loss: 1419.8587513640523\n",
      "epoch: 0, step: 10330, loss: 1421.393519319594\n",
      "epoch: 0, step: 10340, loss: 1422.462720490992\n",
      "epoch: 0, step: 10350, loss: 1423.3422993719578\n",
      "epoch: 0, step: 10360, loss: 1424.2846438288689\n",
      "epoch: 0, step: 10370, loss: 1425.1638240218163\n",
      "epoch: 0, step: 10380, loss: 1426.4919069185853\n",
      "epoch: 0, step: 10390, loss: 1427.462704770267\n",
      "epoch: 0, step: 10400, loss: 1428.357262402773\n",
      "epoch: 0, step: 10410, loss: 1429.3399904221296\n",
      "epoch: 0, step: 10420, loss: 1430.5618673115969\n",
      "epoch: 0, step: 10430, loss: 1431.7229749634862\n",
      "epoch: 0, step: 10440, loss: 1432.970684222877\n",
      "epoch: 0, step: 10450, loss: 1434.3497169911861\n",
      "epoch: 0, step: 10460, loss: 1435.328681126237\n",
      "epoch: 0, step: 10470, loss: 1436.4707429483533\n",
      "epoch: 0, step: 10480, loss: 1438.4136205911636\n",
      "epoch: 0, step: 10490, loss: 1439.3049915879965\n",
      "epoch: 0, step: 10500, loss: 1440.462038770318\n",
      "epoch: 0, step: 10510, loss: 1441.5322833433747\n",
      "epoch: 0, step: 10520, loss: 1442.6054891124368\n",
      "epoch: 0, step: 10530, loss: 1443.945442341268\n",
      "epoch: 0, step: 10540, loss: 1445.1330657601357\n",
      "epoch: 0, step: 10550, loss: 1446.3645248264074\n",
      "epoch: 0, step: 10560, loss: 1447.3089338093996\n",
      "epoch: 0, step: 10570, loss: 1448.4761134386063\n",
      "epoch: 0, step: 10580, loss: 1450.2716334313154\n",
      "epoch: 0, step: 10590, loss: 1451.3855889067054\n",
      "epoch: 0, step: 10600, loss: 1452.6080176532269\n",
      "epoch: 0, step: 10610, loss: 1453.6722155287862\n",
      "epoch: 0, step: 10620, loss: 1456.904368646443\n",
      "epoch: 0, step: 10630, loss: 1458.5959988161922\n",
      "epoch: 0, step: 10640, loss: 1460.020619392395\n",
      "epoch: 0, step: 10650, loss: 1461.860436655581\n",
      "epoch: 0, step: 10660, loss: 1463.0421642363071\n",
      "epoch: 0, step: 10670, loss: 1463.9689142033458\n",
      "epoch: 0, step: 10680, loss: 1464.9115711599588\n",
      "epoch: 0, step: 10690, loss: 1466.1588013544679\n",
      "epoch: 0, step: 10700, loss: 1468.9306902214885\n",
      "epoch: 0, step: 10710, loss: 1471.139657959342\n",
      "epoch: 0, step: 10720, loss: 1472.8131281659007\n",
      "epoch: 0, step: 10730, loss: 1474.1842992901802\n",
      "epoch: 0, step: 10740, loss: 1475.7104969322681\n",
      "epoch: 0, step: 10750, loss: 1477.0848901420832\n",
      "epoch: 0, step: 10760, loss: 1478.1073775216937\n",
      "epoch: 0, step: 10770, loss: 1479.4334020242095\n",
      "epoch: 0, step: 10780, loss: 1481.2101858779788\n",
      "epoch: 0, step: 10790, loss: 1483.2647507414222\n",
      "epoch: 0, step: 10800, loss: 1485.122017852962\n",
      "epoch: 0, step: 10810, loss: 1486.3529774546623\n",
      "epoch: 0, step: 10820, loss: 1487.6831341534853\n",
      "epoch: 0, step: 10830, loss: 1488.8450131565332\n",
      "epoch: 0, step: 10840, loss: 1490.168792694807\n",
      "epoch: 0, step: 10850, loss: 1490.9485272318125\n",
      "epoch: 0, step: 10860, loss: 1492.748950369656\n",
      "epoch: 0, step: 10870, loss: 1494.6151828840375\n",
      "epoch: 0, step: 10880, loss: 1495.8170049488544\n",
      "epoch: 0, step: 10890, loss: 1497.0563682317734\n",
      "epoch: 0, step: 10900, loss: 1498.0774182006717\n",
      "epoch: 0, step: 10910, loss: 1499.2013539597392\n",
      "epoch: 0, step: 10920, loss: 1500.7053127512336\n",
      "epoch: 0, step: 10930, loss: 1501.594699293375\n",
      "epoch: 0, step: 10940, loss: 1502.8026094585657\n",
      "epoch: 0, step: 10950, loss: 1503.884665004909\n",
      "epoch: 0, step: 10960, loss: 1504.9097976908088\n",
      "epoch: 0, step: 10970, loss: 1506.4789694398642\n",
      "epoch: 0, step: 10980, loss: 1508.069588497281\n",
      "epoch: 0, step: 10990, loss: 1510.0848800092936\n",
      "epoch: 0, step: 11000, loss: 1511.9917229190469\n",
      "epoch: 0, step: 11010, loss: 1513.1431138068438\n",
      "epoch: 0, step: 11020, loss: 1514.5351179391146\n",
      "epoch: 0, step: 11030, loss: 1516.0036836043\n",
      "epoch: 0, step: 11040, loss: 1517.6265136152506\n",
      "epoch: 0, step: 11050, loss: 1518.909231454134\n",
      "epoch: 0, step: 11060, loss: 1519.9449820593\n",
      "epoch: 0, step: 11070, loss: 1521.1280641630292\n",
      "epoch: 0, step: 11080, loss: 1522.2246260717511\n",
      "epoch: 0, step: 11090, loss: 1523.2772080004215\n",
      "epoch: 0, step: 11100, loss: 1524.6879793703556\n",
      "epoch: 0, step: 11110, loss: 1526.2702896669507\n",
      "epoch: 0, step: 11120, loss: 1527.8530858308077\n",
      "epoch: 0, step: 11130, loss: 1528.717478454113\n",
      "epoch: 0, step: 11140, loss: 1529.576171129942\n",
      "epoch: 0, step: 11150, loss: 1531.2550705820322\n",
      "epoch: 0, step: 11160, loss: 1532.572350539267\n",
      "epoch: 0, step: 11170, loss: 1533.3341258689761\n",
      "epoch: 0, step: 11180, loss: 1534.3476992473006\n",
      "epoch: 0, step: 11190, loss: 1535.2297230958939\n",
      "epoch: 0, step: 11200, loss: 1536.578152231872\n",
      "epoch: 0, step: 11210, loss: 1537.770807750523\n",
      "epoch: 0, step: 11220, loss: 1539.4415675923228\n",
      "epoch: 0, step: 11230, loss: 1540.408583112061\n",
      "epoch: 0, step: 11240, loss: 1541.2780093476176\n",
      "epoch: 0, step: 11250, loss: 1542.1561016067863\n",
      "epoch: 0, step: 11260, loss: 1543.1116206645966\n",
      "epoch: 0, step: 11270, loss: 1543.9896350353956\n",
      "epoch: 0, step: 11280, loss: 1545.119269862771\n",
      "epoch: 0, step: 11290, loss: 1546.4139631092548\n",
      "epoch: 0, step: 11300, loss: 1547.3333096951246\n",
      "epoch: 0, step: 11310, loss: 1548.634935438633\n",
      "epoch: 0, step: 11320, loss: 1549.686099536717\n",
      "epoch: 0, step: 11330, loss: 1550.5569699704647\n",
      "epoch: 0, step: 11340, loss: 1551.7865249738097\n",
      "epoch: 0, step: 11350, loss: 1552.9706887230277\n",
      "epoch: 0, step: 11360, loss: 1554.626879915595\n",
      "epoch: 0, step: 11370, loss: 1555.7037133052945\n",
      "epoch: 0, step: 11380, loss: 1556.614848665893\n",
      "epoch: 0, step: 11390, loss: 1557.7765396088362\n",
      "epoch: 0, step: 11400, loss: 1559.0870122686028\n",
      "epoch: 0, step: 11410, loss: 1560.6475980356336\n",
      "epoch: 0, step: 11420, loss: 1561.909866452217\n",
      "epoch: 0, step: 11430, loss: 1563.604632601142\n",
      "epoch: 0, step: 11440, loss: 1564.6963394358754\n",
      "epoch: 0, step: 11450, loss: 1565.6071229577065\n",
      "epoch: 0, step: 11460, loss: 1566.885448396206\n",
      "epoch: 0, step: 11470, loss: 1568.1013440191746\n",
      "epoch: 0, step: 11480, loss: 1569.4551333859563\n",
      "epoch: 0, step: 11490, loss: 1570.83216316998\n",
      "epoch: 0, step: 11500, loss: 1571.8306943550706\n",
      "epoch: 0, step: 11510, loss: 1573.2810967415571\n",
      "epoch: 0, step: 11520, loss: 1574.4528307765722\n",
      "epoch: 0, step: 11530, loss: 1575.7876198515296\n",
      "epoch: 0, step: 11540, loss: 1577.3990246728063\n",
      "epoch: 0, step: 11550, loss: 1578.5468326807022\n",
      "epoch: 0, step: 11560, loss: 1579.6209260746837\n",
      "epoch: 0, step: 11570, loss: 1581.39427138865\n",
      "epoch: 0, step: 11580, loss: 1582.6672293543816\n",
      "epoch: 0, step: 11590, loss: 1583.8302796334028\n",
      "epoch: 0, step: 11600, loss: 1585.2345861941576\n",
      "epoch: 0, step: 11610, loss: 1586.882542014122\n",
      "epoch: 0, step: 11620, loss: 1587.9099841192365\n",
      "epoch: 0, step: 11630, loss: 1588.9924476519227\n",
      "epoch: 0, step: 11640, loss: 1590.1065569072962\n",
      "epoch: 0, step: 11650, loss: 1591.3751734495163\n",
      "epoch: 0, step: 11660, loss: 1593.1841387674212\n",
      "epoch: 0, step: 11670, loss: 1594.4270070195198\n",
      "epoch: 0, step: 11680, loss: 1595.5529969111085\n",
      "epoch: 0, step: 11690, loss: 1596.4679310321808\n",
      "epoch: 0, step: 11700, loss: 1597.3139039427042\n",
      "epoch: 0, step: 11710, loss: 1598.8211284726858\n",
      "epoch: 0, step: 11720, loss: 1600.1866344064474\n",
      "epoch: 0, step: 11730, loss: 1601.4353345558047\n",
      "epoch: 0, step: 11740, loss: 1603.4953231811523\n",
      "epoch: 0, step: 11750, loss: 1606.083265773952\n",
      "epoch: 0, step: 11760, loss: 1607.7308554649353\n",
      "epoch: 0, step: 11770, loss: 1608.9094732254744\n",
      "epoch: 0, step: 11780, loss: 1609.9504544734955\n",
      "epoch: 0, step: 11790, loss: 1610.9606524184346\n",
      "epoch: 0, step: 11800, loss: 1612.1075615510345\n",
      "epoch: 0, step: 11810, loss: 1613.1213374510407\n",
      "epoch: 0, step: 11820, loss: 1614.0889579728246\n",
      "epoch: 0, step: 11830, loss: 1615.0480065643787\n",
      "epoch: 0, step: 11840, loss: 1616.0581194534898\n",
      "epoch: 0, step: 11850, loss: 1616.833271406591\n",
      "epoch: 0, step: 11860, loss: 1617.6892768666148\n",
      "epoch: 0, step: 11870, loss: 1619.000374108553\n",
      "epoch: 0, step: 11880, loss: 1620.0842367336154\n",
      "epoch: 0, step: 11890, loss: 1621.4645588621497\n",
      "epoch: 0, step: 11900, loss: 1622.6861648783088\n",
      "epoch: 0, step: 11910, loss: 1624.0025922507048\n",
      "epoch: 0, step: 11920, loss: 1625.2388417348266\n",
      "epoch: 0, step: 11930, loss: 1626.0895120501518\n",
      "epoch: 0, step: 11940, loss: 1626.9681485146284\n",
      "epoch: 0, step: 11950, loss: 1628.1935161352158\n",
      "epoch: 0, step: 11960, loss: 1630.6046025380492\n",
      "epoch: 0, step: 11970, loss: 1631.7435487732291\n",
      "epoch: 0, step: 11980, loss: 1632.8065493777394\n",
      "epoch: 0, step: 11990, loss: 1633.8379950746894\n",
      "epoch: 0, step: 12000, loss: 1634.8163445070386\n",
      "epoch: 0, step: 12010, loss: 1635.9512983486056\n",
      "epoch: 0, step: 12020, loss: 1638.0499257594347\n",
      "epoch: 0, step: 12030, loss: 1639.062075264752\n",
      "epoch: 0, step: 12040, loss: 1640.3521225303411\n",
      "epoch: 0, step: 12050, loss: 1641.5444281622767\n",
      "epoch: 0, step: 12060, loss: 1642.3499278202653\n",
      "epoch: 0, step: 12070, loss: 1643.700905174017\n",
      "epoch: 0, step: 12080, loss: 1645.7105390951037\n",
      "epoch: 0, step: 12090, loss: 1647.9517794549465\n",
      "epoch: 0, step: 12100, loss: 1648.9838791713119\n",
      "epoch: 0, step: 12110, loss: 1649.9435687065125\n",
      "epoch: 0, step: 12120, loss: 1651.0917721614242\n",
      "epoch: 0, step: 12130, loss: 1652.6165348142385\n",
      "epoch: 0, step: 12140, loss: 1653.768844410777\n",
      "epoch: 0, step: 12150, loss: 1654.8129879683256\n",
      "epoch: 0, step: 12160, loss: 1657.404354609549\n",
      "epoch: 0, step: 12170, loss: 1659.6962266042829\n",
      "epoch: 0, step: 12180, loss: 1660.648551762104\n",
      "epoch: 0, step: 12190, loss: 1662.3876858949661\n",
      "epoch: 0, step: 12200, loss: 1663.1990931853652\n",
      "epoch: 0, step: 12210, loss: 1664.6876512691379\n",
      "epoch: 0, step: 12220, loss: 1666.0659145191312\n",
      "epoch: 0, step: 12230, loss: 1666.9750378876925\n",
      "epoch: 0, step: 12240, loss: 1668.29811386019\n",
      "epoch: 0, step: 12250, loss: 1669.2189545854926\n",
      "epoch: 0, step: 12260, loss: 1670.3485580310225\n",
      "epoch: 0, step: 12270, loss: 1671.5592052713037\n",
      "epoch: 0, step: 12280, loss: 1673.7930291071534\n",
      "epoch: 0, step: 12290, loss: 1674.8924581184983\n",
      "epoch: 0, step: 12300, loss: 1676.0513684302568\n",
      "epoch: 0, step: 12310, loss: 1677.0361159592867\n",
      "epoch: 0, step: 12320, loss: 1678.0396499186754\n",
      "epoch: 0, step: 12330, loss: 1679.42106603086\n",
      "epoch: 0, step: 12340, loss: 1680.9301047474146\n",
      "epoch: 0, step: 12350, loss: 1681.8759401738644\n",
      "epoch: 0, step: 12360, loss: 1682.9173492640257\n",
      "epoch: 0, step: 12370, loss: 1684.1580748558044\n",
      "epoch: 0, step: 12380, loss: 1685.1732937842607\n",
      "epoch: 0, step: 12390, loss: 1687.20678909868\n",
      "epoch: 0, step: 12400, loss: 1688.6287426799536\n",
      "epoch: 0, step: 12410, loss: 1689.6112688034773\n",
      "epoch: 0, step: 12420, loss: 1690.7727275714278\n",
      "epoch: 0, step: 12430, loss: 1692.1753618121147\n",
      "epoch: 0, step: 12440, loss: 1693.2992413267493\n",
      "epoch: 0, step: 12450, loss: 1694.368812687695\n",
      "epoch: 0, step: 12460, loss: 1695.338530138135\n",
      "epoch: 0, step: 12470, loss: 1696.2348962128162\n",
      "epoch: 0, step: 12480, loss: 1697.679560624063\n",
      "epoch: 0, step: 12490, loss: 1698.7388123869896\n",
      "epoch: 0, step: 12500, loss: 1699.6518565937877\n",
      "epoch: 0, step: 12510, loss: 1700.7688927575946\n",
      "epoch: 0, step: 12520, loss: 1702.189531557262\n",
      "epoch: 0, step: 12530, loss: 1703.3532688766718\n",
      "epoch: 0, step: 12540, loss: 1704.72816272825\n",
      "epoch: 0, step: 12550, loss: 1705.8652628362179\n",
      "epoch: 0, step: 12560, loss: 1707.018641717732\n",
      "epoch: 0, step: 12570, loss: 1708.5696169510484\n",
      "epoch: 0, step: 12580, loss: 1709.706542879343\n",
      "epoch: 0, step: 12590, loss: 1710.747645162046\n",
      "epoch: 0, step: 12600, loss: 1712.738641165197\n",
      "epoch: 0, step: 12610, loss: 1713.741393327713\n",
      "epoch: 0, step: 12620, loss: 1714.6442841514945\n",
      "epoch: 0, step: 12630, loss: 1715.8021313399076\n",
      "epoch: 0, step: 12640, loss: 1718.2441532164812\n",
      "epoch: 0, step: 12650, loss: 1723.2029047757387\n",
      "epoch: 0, step: 12660, loss: 1726.1680148914456\n",
      "epoch: 0, step: 12670, loss: 1727.3619001805782\n",
      "epoch: 0, step: 12680, loss: 1729.1895611584187\n",
      "epoch: 0, step: 12690, loss: 1730.2961626201868\n",
      "epoch: 0, step: 12700, loss: 1731.6382470056415\n",
      "epoch: 0, step: 12710, loss: 1732.544081620872\n",
      "epoch: 0, step: 12720, loss: 1733.5648109465837\n",
      "epoch: 0, step: 12730, loss: 1734.3751499801874\n",
      "epoch: 0, step: 12740, loss: 1735.6331661120057\n",
      "epoch: 0, step: 12750, loss: 1737.135228946805\n",
      "epoch: 0, step: 12760, loss: 1738.9913163036108\n",
      "epoch: 0, step: 12770, loss: 1739.874845251441\n",
      "epoch: 0, step: 12780, loss: 1740.9669156819582\n",
      "epoch: 0, step: 12790, loss: 1742.8751397877932\n",
      "epoch: 0, step: 12800, loss: 1743.8537253811955\n",
      "epoch: 0, step: 12810, loss: 1745.1956591308117\n",
      "epoch: 0, step: 12820, loss: 1746.100656747818\n",
      "epoch: 0, step: 12830, loss: 1747.1918471455574\n",
      "epoch: 0, step: 12840, loss: 1748.4281518682837\n",
      "epoch: 0, step: 12850, loss: 1749.4092524275184\n",
      "epoch: 0, step: 12860, loss: 1751.4274113923311\n",
      "epoch: 0, step: 12870, loss: 1752.3401750326157\n",
      "epoch: 0, step: 12880, loss: 1753.4185629934072\n",
      "epoch: 0, step: 12890, loss: 1754.4194620400667\n",
      "epoch: 0, step: 12900, loss: 1756.1546417623758\n",
      "epoch: 0, step: 12910, loss: 1758.083447597921\n",
      "epoch: 0, step: 12920, loss: 1759.267950154841\n",
      "epoch: 0, step: 12930, loss: 1760.141681626439\n",
      "epoch: 0, step: 12940, loss: 1761.3694148734212\n",
      "epoch: 0, step: 12950, loss: 1762.6860874071717\n",
      "epoch: 0, step: 12960, loss: 1764.730068422854\n",
      "epoch: 0, step: 12970, loss: 1765.642121180892\n",
      "epoch: 0, step: 12980, loss: 1766.6789133027196\n",
      "epoch: 0, step: 12990, loss: 1767.9085219204426\n",
      "epoch: 0, step: 13000, loss: 1769.0095463395119\n",
      "epoch: 0, step: 13010, loss: 1770.3511734604836\n",
      "epoch: 0, step: 13020, loss: 1771.3892978355289\n",
      "epoch: 0, step: 13030, loss: 1772.324691168964\n",
      "epoch: 0, step: 13040, loss: 1773.4348998069763\n",
      "epoch: 0, step: 13050, loss: 1774.3548232614994\n",
      "epoch: 0, step: 13060, loss: 1775.1393642947078\n",
      "epoch: 0, step: 13070, loss: 1776.2970138937235\n",
      "epoch: 0, step: 13080, loss: 1777.3167835697532\n",
      "epoch: 0, step: 13090, loss: 1778.504689656198\n",
      "epoch: 0, step: 13100, loss: 1779.2557600662112\n",
      "epoch: 0, step: 13110, loss: 1780.6921708658338\n",
      "epoch: 0, step: 13120, loss: 1781.6049162968993\n",
      "epoch: 0, step: 13130, loss: 1782.4580140933394\n",
      "epoch: 0, step: 13140, loss: 1783.201641768217\n",
      "epoch: 0, step: 13150, loss: 1784.6587072759867\n",
      "epoch: 0, step: 13160, loss: 1786.077559567988\n",
      "epoch: 0, step: 13170, loss: 1787.166873268783\n",
      "epoch: 0, step: 13180, loss: 1787.9410494863987\n",
      "epoch: 0, step: 13190, loss: 1788.800826177001\n",
      "epoch: 0, step: 13200, loss: 1789.5920093283057\n",
      "epoch: 0, step: 13210, loss: 1790.4436879083514\n",
      "epoch: 0, step: 13220, loss: 1792.139578744769\n",
      "epoch: 0, step: 13230, loss: 1793.6483622193336\n",
      "epoch: 0, step: 13240, loss: 1794.8381593823433\n",
      "epoch: 0, step: 13250, loss: 1795.7005654349923\n",
      "epoch: 0, step: 13260, loss: 1796.4828895032406\n",
      "epoch: 0, step: 13270, loss: 1797.3238534778357\n",
      "epoch: 0, step: 13280, loss: 1798.5053970590234\n",
      "epoch: 0, step: 13290, loss: 1799.4112800434232\n",
      "epoch: 0, step: 13300, loss: 1800.1594679653645\n",
      "epoch: 0, step: 13310, loss: 1801.4138239175081\n",
      "epoch: 0, step: 13320, loss: 1802.7458480969071\n",
      "epoch: 0, step: 13330, loss: 1803.7742386385798\n",
      "epoch: 0, step: 13340, loss: 1804.7972406372428\n",
      "epoch: 0, step: 13350, loss: 1806.3066348731518\n",
      "epoch: 0, step: 13360, loss: 1808.0065910220146\n",
      "epoch: 0, step: 13370, loss: 1808.9369944781065\n",
      "epoch: 0, step: 13380, loss: 1809.9432705268264\n",
      "epoch: 0, step: 13390, loss: 1811.1060640588403\n",
      "epoch: 0, step: 13400, loss: 1812.239366054535\n",
      "epoch: 0, step: 13410, loss: 1813.201688066125\n",
      "epoch: 0, step: 13420, loss: 1814.3773362338543\n",
      "epoch: 0, step: 13430, loss: 1815.2503262385726\n",
      "epoch: 0, step: 13440, loss: 1816.0990523844957\n",
      "epoch: 0, step: 13450, loss: 1817.2323717102408\n",
      "epoch: 0, step: 13460, loss: 1818.3114256635308\n",
      "epoch: 0, step: 13470, loss: 1819.2512888163328\n",
      "epoch: 0, step: 13480, loss: 1820.570490680635\n",
      "epoch: 0, step: 13490, loss: 1821.7434929311275\n",
      "epoch: 0, step: 13500, loss: 1822.803061813116\n",
      "epoch: 0, step: 13510, loss: 1824.25626565516\n",
      "epoch: 0, step: 13520, loss: 1825.8127026930451\n",
      "epoch: 0, step: 13530, loss: 1827.103828497231\n",
      "epoch: 0, step: 13540, loss: 1827.954237766564\n",
      "epoch: 0, step: 13550, loss: 1829.125363856554\n",
      "epoch: 0, step: 13560, loss: 1830.4110292494297\n",
      "epoch: 0, step: 13570, loss: 1831.3803065344691\n",
      "epoch: 0, step: 13580, loss: 1832.7261510416865\n",
      "epoch: 0, step: 13590, loss: 1833.8138573616743\n",
      "epoch: 0, step: 13600, loss: 1834.8858755603433\n",
      "epoch: 0, step: 13610, loss: 1836.4000263735652\n",
      "epoch: 0, step: 13620, loss: 1838.1243543550372\n",
      "epoch: 0, step: 13630, loss: 1839.401955857873\n",
      "epoch: 0, step: 13640, loss: 1840.3300359919667\n",
      "epoch: 0, step: 13650, loss: 1841.3521916866302\n",
      "epoch: 0, step: 13660, loss: 1842.3188861459494\n",
      "epoch: 0, step: 13670, loss: 1843.545858770609\n",
      "epoch: 0, step: 13680, loss: 1844.7388249114156\n",
      "epoch: 0, step: 13690, loss: 1845.973122395575\n",
      "epoch: 0, step: 13700, loss: 1847.3620723336935\n",
      "epoch: 0, step: 13710, loss: 1848.7453386858106\n",
      "epoch: 0, step: 13720, loss: 1849.6282280161977\n",
      "epoch: 0, step: 13730, loss: 1850.8660477772355\n",
      "epoch: 0, step: 13740, loss: 1851.750101722777\n",
      "epoch: 0, step: 13750, loss: 1853.459465302527\n",
      "epoch: 0, step: 13760, loss: 1855.123691931367\n",
      "epoch: 0, step: 13770, loss: 1856.3497142866254\n",
      "epoch: 0, step: 13780, loss: 1858.0280178710818\n",
      "epoch: 0, step: 13790, loss: 1859.2008556872606\n",
      "epoch: 0, step: 13800, loss: 1860.4546933844686\n",
      "epoch: 0, step: 13810, loss: 1861.476039968431\n",
      "epoch: 0, step: 13820, loss: 1862.563390083611\n",
      "epoch: 0, step: 13830, loss: 1863.755389533937\n",
      "epoch: 0, step: 13840, loss: 1864.839808627963\n",
      "epoch: 0, step: 13850, loss: 1865.8768675476313\n",
      "epoch: 0, step: 13860, loss: 1866.9960527047515\n",
      "epoch: 0, step: 13870, loss: 1867.800922818482\n",
      "epoch: 0, step: 13880, loss: 1870.2068386226892\n",
      "epoch: 0, step: 13890, loss: 1871.5761404260993\n",
      "epoch: 0, step: 13900, loss: 1872.6040994003415\n",
      "epoch: 0, step: 13910, loss: 1873.4974407404661\n",
      "epoch: 0, step: 13920, loss: 1874.4416902959347\n",
      "epoch: 0, step: 13930, loss: 1875.5771113857627\n",
      "epoch: 0, step: 13940, loss: 1877.5270833671093\n",
      "epoch: 0, step: 13950, loss: 1878.6924092248082\n",
      "epoch: 0, step: 13960, loss: 1880.1149863824248\n",
      "epoch: 0, step: 13970, loss: 1881.157331675291\n",
      "epoch: 0, step: 13980, loss: 1882.1817487329245\n",
      "epoch: 0, step: 13990, loss: 1883.1558850705624\n",
      "epoch: 0, step: 14000, loss: 1884.4355084300041\n",
      "epoch: 0, step: 14010, loss: 1885.3890632763505\n",
      "epoch: 0, step: 14020, loss: 1887.4013391509652\n",
      "epoch: 0, step: 14030, loss: 1888.3501433730125\n",
      "epoch: 0, step: 14040, loss: 1889.504713267088\n",
      "epoch: 0, step: 14050, loss: 1890.639656342566\n",
      "epoch: 0, step: 14060, loss: 1891.738212749362\n",
      "epoch: 0, step: 14070, loss: 1892.8687977716327\n",
      "epoch: 0, step: 14080, loss: 1894.2677908688784\n",
      "epoch: 0, step: 14090, loss: 1895.6859999671578\n",
      "epoch: 0, step: 14100, loss: 1896.7145633921027\n",
      "epoch: 0, step: 14110, loss: 1897.7109383493662\n",
      "epoch: 0, step: 14120, loss: 1898.8858913257718\n",
      "epoch: 0, step: 14130, loss: 1899.9491812363267\n",
      "epoch: 0, step: 14140, loss: 1900.8400413766503\n",
      "epoch: 0, step: 14150, loss: 1901.9967705830932\n",
      "epoch: 0, step: 14160, loss: 1903.0321031287313\n",
      "epoch: 0, step: 14170, loss: 1904.4927997514606\n",
      "epoch: 0, step: 14180, loss: 1905.5741418972611\n",
      "epoch: 0, step: 14190, loss: 1906.4797225892544\n",
      "epoch: 0, step: 14200, loss: 1907.7556126341224\n",
      "epoch: 0, step: 14210, loss: 1908.7465181797743\n",
      "epoch: 0, step: 14220, loss: 1909.6757866442204\n",
      "epoch: 0, step: 14230, loss: 1910.7388183623552\n",
      "epoch: 0, step: 14240, loss: 1911.7927425950766\n",
      "epoch: 0, step: 14250, loss: 1912.5895586162806\n",
      "epoch: 0, step: 14260, loss: 1913.833596482873\n",
      "epoch: 0, step: 14270, loss: 1914.8761742338538\n",
      "epoch: 0, step: 14280, loss: 1915.7557309120893\n",
      "epoch: 0, step: 14290, loss: 1917.821337327361\n",
      "epoch: 0, step: 14300, loss: 1918.9889312908053\n",
      "epoch: 0, step: 14310, loss: 1919.9446472600102\n",
      "epoch: 0, step: 14320, loss: 1920.8157779350877\n",
      "epoch: 0, step: 14330, loss: 1921.8529533222318\n",
      "epoch: 0, step: 14340, loss: 1922.9953821897507\n",
      "epoch: 0, step: 14350, loss: 1924.0246258154511\n",
      "epoch: 0, step: 14360, loss: 1924.9425438195467\n",
      "epoch: 0, step: 14370, loss: 1926.0820119082928\n",
      "epoch: 0, step: 14380, loss: 1927.1889990344644\n",
      "epoch: 0, step: 14390, loss: 1928.240403585136\n",
      "epoch: 0, step: 14400, loss: 1929.169155061245\n",
      "epoch: 0, step: 14410, loss: 1930.5782098397613\n",
      "epoch: 0, step: 14420, loss: 1931.637063279748\n",
      "epoch: 0, step: 14430, loss: 1932.5977110415697\n",
      "epoch: 0, step: 14440, loss: 1933.4081508815289\n",
      "epoch: 0, step: 14450, loss: 1934.3690789341927\n",
      "epoch: 0, step: 14460, loss: 1935.103465180844\n",
      "epoch: 0, step: 14470, loss: 1936.9145421423018\n",
      "epoch: 0, step: 14480, loss: 1938.5973325558007\n",
      "epoch: 0, step: 14490, loss: 1939.675351332873\n",
      "epoch: 0, step: 14500, loss: 1941.1435190625489\n",
      "epoch: 0, step: 14510, loss: 1941.9431664980948\n",
      "epoch: 0, step: 14520, loss: 1942.881298724562\n",
      "epoch: 0, step: 14530, loss: 1943.7693021409214\n",
      "epoch: 0, step: 14540, loss: 1944.6951762624085\n",
      "epoch: 0, step: 14550, loss: 1945.6115855686367\n",
      "epoch: 0, step: 14560, loss: 1947.282395426184\n",
      "epoch: 0, step: 14570, loss: 1949.0872502066195\n",
      "epoch: 0, step: 14580, loss: 1950.4193856082857\n",
      "epoch: 0, step: 14590, loss: 1952.1190452985466\n",
      "epoch: 0, step: 14600, loss: 1955.7854673229158\n",
      "epoch: 0, step: 14610, loss: 1959.8209430985153\n",
      "epoch: 0, step: 14620, loss: 1962.6058728732169\n",
      "epoch: 0, step: 14630, loss: 1965.1145843751729\n",
      "epoch: 0, step: 14640, loss: 1966.6916542090476\n",
      "epoch: 0, step: 14650, loss: 1968.3858805857599\n",
      "epoch: 0, step: 14660, loss: 1970.4982173331082\n",
      "epoch: 0, step: 14670, loss: 1973.5037429071963\n",
      "epoch: 0, step: 14680, loss: 1975.4850330092013\n",
      "epoch: 0, step: 14690, loss: 1976.5813550837338\n",
      "epoch: 0, step: 14700, loss: 1977.5574568547308\n",
      "epoch: 0, step: 14710, loss: 1978.5664618350565\n",
      "epoch: 0, step: 14720, loss: 1979.8776094205678\n",
      "epoch: 0, step: 14730, loss: 1981.0122167281806\n",
      "epoch: 0, step: 14740, loss: 1982.1392450667918\n",
      "epoch: 0, step: 14750, loss: 1983.0145155452192\n",
      "epoch: 0, step: 14760, loss: 1984.0030188076198\n",
      "epoch: 0, step: 14770, loss: 1986.0208550356328\n",
      "epoch: 0, step: 14780, loss: 1987.3384379930794\n",
      "epoch: 0, step: 14790, loss: 1988.4104337058961\n",
      "epoch: 0, step: 14800, loss: 1989.3459483720362\n",
      "epoch: 0, step: 14810, loss: 1990.1610123179853\n",
      "epoch: 0, step: 14820, loss: 1991.1000726632774\n",
      "epoch: 0, step: 14830, loss: 1992.0024722106755\n",
      "epoch: 0, step: 14840, loss: 1992.9223433993757\n",
      "epoch: 0, step: 14850, loss: 1993.8346749059856\n",
      "epoch: 0, step: 14860, loss: 1994.679813992232\n",
      "epoch: 0, step: 14870, loss: 1996.8690601848066\n",
      "epoch: 0, step: 14880, loss: 1997.966727744788\n",
      "epoch: 0, step: 14890, loss: 1999.1860642023385\n",
      "epoch: 0, step: 14900, loss: 2003.4952493794262\n",
      "epoch: 0, step: 14910, loss: 2005.3999903686345\n",
      "epoch: 0, step: 14920, loss: 2006.4223221503198\n",
      "epoch: 0, step: 14930, loss: 2007.4756163023412\n",
      "epoch: 0, step: 14940, loss: 2008.5677741654217\n",
      "epoch: 0, step: 14950, loss: 2009.5351168476045\n",
      "epoch: 0, step: 14960, loss: 2010.8688283450902\n",
      "epoch: 0, step: 14970, loss: 2012.3368861190975\n",
      "epoch: 0, step: 14980, loss: 2013.5395545475185\n",
      "epoch: 0, step: 14990, loss: 2014.4422131888568\n",
      "epoch: 0, step: 15000, loss: 2015.6023001559079\n",
      "epoch: 0, step: 15010, loss: 2016.6607647277415\n",
      "epoch: 0, step: 15020, loss: 2017.5382195450366\n",
      "epoch: 0, step: 15030, loss: 2018.5165449790657\n",
      "epoch: 0, step: 15040, loss: 2019.6761357448995\n",
      "epoch: 0, step: 15050, loss: 2020.6371769420803\n",
      "epoch: 0, step: 15060, loss: 2021.6642294190824\n",
      "epoch: 0, step: 15070, loss: 2022.593148585409\n",
      "epoch: 0, step: 15080, loss: 2023.74874580279\n",
      "epoch: 0, step: 15090, loss: 2024.9217462651432\n",
      "epoch: 0, step: 15100, loss: 2025.8578336052597\n",
      "epoch: 0, step: 15110, loss: 2026.806749548763\n",
      "epoch: 0, step: 15120, loss: 2028.1484478078783\n",
      "epoch: 0, step: 15130, loss: 2029.346115346998\n",
      "epoch: 0, step: 15140, loss: 2030.586821448058\n",
      "epoch: 0, step: 15150, loss: 2031.7441948764026\n",
      "epoch: 0, step: 15160, loss: 2032.7803239114583\n",
      "epoch: 0, step: 15170, loss: 2033.730421539396\n",
      "epoch: 0, step: 15180, loss: 2035.711261112243\n",
      "epoch: 0, step: 15190, loss: 2036.8423827923834\n",
      "epoch: 0, step: 15200, loss: 2038.3111247457564\n",
      "epoch: 0, step: 15210, loss: 2039.0727229602635\n",
      "epoch: 0, step: 15220, loss: 2040.070741545409\n",
      "epoch: 0, step: 15230, loss: 2041.1174249909818\n",
      "epoch: 0, step: 15240, loss: 2042.5199974738061\n",
      "epoch: 0, step: 15250, loss: 2043.4983546324074\n",
      "epoch: 0, step: 15260, loss: 2044.6408082805574\n",
      "epoch: 0, step: 15270, loss: 2046.583349559456\n",
      "epoch: 0, step: 15280, loss: 2047.905322726816\n",
      "epoch: 0, step: 15290, loss: 2049.7752655483782\n",
      "epoch: 0, step: 15300, loss: 2051.09181464836\n",
      "epoch: 0, step: 15310, loss: 2052.784635987133\n",
      "epoch: 0, step: 15320, loss: 2053.762794096023\n",
      "epoch: 0, step: 15330, loss: 2054.76614734903\n",
      "epoch: 0, step: 15340, loss: 2056.246082659811\n",
      "epoch: 0, step: 15350, loss: 2057.078627806157\n",
      "epoch: 0, step: 15360, loss: 2058.133605647832\n",
      "epoch: 0, step: 15370, loss: 2059.041682500392\n",
      "epoch: 0, step: 15380, loss: 2060.2377516664565\n",
      "epoch: 0, step: 15390, loss: 2061.2541590072215\n",
      "epoch: 0, step: 15400, loss: 2062.3818462379277\n",
      "epoch: 0, step: 15410, loss: 2063.616745200008\n",
      "epoch: 0, step: 15420, loss: 2064.888242390007\n",
      "epoch: 0, step: 15430, loss: 2066.2499260641634\n",
      "epoch: 0, step: 15440, loss: 2067.199314709753\n",
      "epoch: 0, step: 15450, loss: 2068.0240905098617\n",
      "epoch: 0, step: 15460, loss: 2068.9674261920154\n",
      "epoch: 0, step: 15470, loss: 2070.067078974098\n",
      "epoch: 0, step: 15480, loss: 2070.853331219405\n",
      "epoch: 0, step: 15490, loss: 2071.947290856391\n",
      "epoch: 0, step: 15500, loss: 2073.0499272085726\n",
      "epoch: 0, step: 15510, loss: 2074.0429807193577\n",
      "epoch: 0, step: 15520, loss: 2075.0045799724758\n",
      "epoch: 0, step: 15530, loss: 2076.0870941616595\n",
      "epoch: 0, step: 15540, loss: 2076.9945381470025\n",
      "epoch: 0, step: 15550, loss: 2078.196025516838\n",
      "epoch: 0, step: 15560, loss: 2079.149882812053\n",
      "epoch: 0, step: 15570, loss: 2080.152367759496\n",
      "epoch: 0, step: 15580, loss: 2080.960875030607\n",
      "epoch: 0, step: 15590, loss: 2082.560634125024\n",
      "epoch: 0, step: 15600, loss: 2083.7648666016757\n",
      "epoch: 0, step: 15610, loss: 2085.969170372933\n",
      "epoch: 0, step: 15620, loss: 2087.1843560375273\n",
      "epoch: 0, step: 15630, loss: 2088.178684029728\n",
      "epoch: 0, step: 15640, loss: 2089.5125771947205\n",
      "epoch: 0, step: 15650, loss: 2090.5252873413265\n",
      "epoch: 0, step: 15660, loss: 2091.401854943484\n",
      "epoch: 0, step: 15670, loss: 2092.6423592679203\n",
      "epoch: 0, step: 15680, loss: 2093.969337489456\n",
      "epoch: 0, step: 15690, loss: 2095.0677228681743\n",
      "epoch: 0, step: 15700, loss: 2096.025462437421\n",
      "epoch: 0, step: 15710, loss: 2096.9649246521294\n",
      "epoch: 0, step: 15720, loss: 2098.0358417741954\n",
      "epoch: 0, step: 15730, loss: 2099.001073602587\n",
      "epoch: 0, step: 15740, loss: 2100.0245820097625\n",
      "epoch: 0, step: 15750, loss: 2101.0581314601004\n",
      "epoch: 0, step: 15760, loss: 2102.2886269129813\n",
      "epoch: 0, step: 15770, loss: 2103.4799258820713\n",
      "epoch: 0, step: 15780, loss: 2104.7561289407313\n",
      "epoch: 0, step: 15790, loss: 2105.886015009135\n",
      "epoch: 0, step: 15800, loss: 2107.3205913566053\n",
      "epoch: 0, step: 15810, loss: 2108.6324887387455\n",
      "epoch: 0, step: 15820, loss: 2109.5396931506693\n",
      "epoch: 0, step: 15830, loss: 2110.8691557087004\n",
      "epoch: 0, step: 15840, loss: 2111.8515360690653\n",
      "epoch: 0, step: 15850, loss: 2112.9728747047484\n",
      "epoch: 0, step: 15860, loss: 2114.4883026368916\n",
      "epoch: 0, step: 15870, loss: 2115.6123994775116\n",
      "epoch: 0, step: 15880, loss: 2116.674124132842\n",
      "epoch: 0, step: 15890, loss: 2117.4435442872345\n",
      "epoch: 0, step: 15900, loss: 2118.929160106927\n",
      "epoch: 0, step: 15910, loss: 2120.0701588876545\n",
      "epoch: 0, step: 15920, loss: 2121.9154953099787\n",
      "epoch: 0, step: 15930, loss: 2123.1240376718342\n",
      "epoch: 0, step: 15940, loss: 2124.770048018545\n",
      "epoch: 0, step: 15950, loss: 2125.6736327819526\n",
      "epoch: 0, step: 15960, loss: 2127.0455624200404\n",
      "epoch: 0, step: 15970, loss: 2128.399775121361\n",
      "epoch: 0, step: 15980, loss: 2129.441476147622\n",
      "epoch: 0, step: 15990, loss: 2130.810478825122\n",
      "epoch: 0, step: 16000, loss: 2131.8769673518836\n",
      "epoch: 0, step: 16010, loss: 2132.6684198491275\n",
      "epoch: 0, step: 16020, loss: 2133.571720983833\n",
      "epoch: 0, step: 16030, loss: 2134.38558710739\n",
      "epoch: 0, step: 16040, loss: 2135.120283138007\n",
      "epoch: 0, step: 16050, loss: 2136.3652762137353\n",
      "epoch: 0, step: 16060, loss: 2137.784381430596\n",
      "epoch: 0, step: 16070, loss: 2139.493389416486\n",
      "epoch: 0, step: 16080, loss: 2140.611263308674\n",
      "epoch: 0, step: 16090, loss: 2141.57812262699\n",
      "epoch: 0, step: 16100, loss: 2142.599872160703\n",
      "epoch: 0, step: 16110, loss: 2143.766264449805\n",
      "epoch: 0, step: 16120, loss: 2145.430901173502\n",
      "epoch: 0, step: 16130, loss: 2146.6072688810527\n",
      "epoch: 0, step: 16140, loss: 2147.6583220250905\n",
      "epoch: 0, step: 16150, loss: 2148.9627376534045\n",
      "epoch: 0, step: 16160, loss: 2150.0869118385017\n",
      "epoch: 0, step: 16170, loss: 2151.26143739745\n",
      "epoch: 0, step: 16180, loss: 2152.0741069056094\n",
      "epoch: 0, step: 16190, loss: 2153.0762679092586\n",
      "epoch: 0, step: 16200, loss: 2154.043874014169\n",
      "epoch: 0, step: 16210, loss: 2154.874482948333\n",
      "epoch: 0, step: 16220, loss: 2155.910913962871\n",
      "epoch: 0, step: 16230, loss: 2156.8574646897614\n",
      "epoch: 0, step: 16240, loss: 2157.7594465501606\n",
      "epoch: 0, step: 16250, loss: 2159.0484058372676\n",
      "epoch: 0, step: 16260, loss: 2160.3478560708463\n",
      "epoch: 0, step: 16270, loss: 2162.36474769935\n",
      "epoch: 0, step: 16280, loss: 2163.235196452588\n",
      "epoch: 0, step: 16290, loss: 2164.2423413582146\n",
      "epoch: 0, step: 16300, loss: 2165.2822241298854\n",
      "epoch: 0, step: 16310, loss: 2166.637496571988\n",
      "epoch: 0, step: 16320, loss: 2167.7586682103574\n",
      "epoch: 0, step: 16330, loss: 2169.1665162704885\n",
      "epoch: 0, step: 16340, loss: 2170.4482606761158\n",
      "epoch: 0, step: 16350, loss: 2171.355269420892\n",
      "epoch: 0, step: 16360, loss: 2172.2746521644294\n",
      "epoch: 0, step: 16370, loss: 2173.7188931442797\n",
      "epoch: 0, step: 16380, loss: 2174.842115033418\n",
      "epoch: 0, step: 16390, loss: 2175.858889337629\n",
      "epoch: 0, step: 16400, loss: 2177.06618020311\n",
      "epoch: 0, step: 16410, loss: 2178.0306962914765\n",
      "epoch: 0, step: 16420, loss: 2179.191920224577\n",
      "epoch: 0, step: 16430, loss: 2180.310010392219\n",
      "epoch: 0, step: 16440, loss: 2181.7822980992496\n",
      "epoch: 0, step: 16450, loss: 2183.808160047978\n",
      "epoch: 0, step: 16460, loss: 2185.04537608847\n",
      "epoch: 0, step: 16470, loss: 2186.281858611852\n",
      "epoch: 0, step: 16480, loss: 2187.840854804963\n",
      "epoch: 0, step: 16490, loss: 2189.2144171260297\n",
      "epoch: 0, step: 16500, loss: 2190.9598219282925\n",
      "epoch: 0, step: 16510, loss: 2192.45773479715\n",
      "epoch: 0, step: 16520, loss: 2194.29024752602\n",
      "epoch: 0, step: 16530, loss: 2195.9965959079564\n",
      "epoch: 0, step: 16540, loss: 2197.366490084678\n",
      "epoch: 0, step: 16550, loss: 2198.3416376151145\n",
      "epoch: 0, step: 16560, loss: 2199.3782935105264\n",
      "epoch: 0, step: 16570, loss: 2200.4656080789864\n",
      "epoch: 0, step: 16580, loss: 2201.58155150339\n",
      "epoch: 0, step: 16590, loss: 2202.7353820987046\n",
      "epoch: 0, step: 16600, loss: 2204.602623645216\n",
      "epoch: 0, step: 16610, loss: 2205.843555737287\n",
      "epoch: 0, step: 16620, loss: 2207.140285860747\n",
      "epoch: 0, step: 16630, loss: 2208.5901369936764\n",
      "epoch: 0, step: 16640, loss: 2210.0858416594565\n",
      "epoch: 0, step: 16650, loss: 2211.0726456306875\n",
      "epoch: 0, step: 16660, loss: 2211.977345947176\n",
      "epoch: 0, step: 16670, loss: 2214.8507759161294\n",
      "epoch: 0, step: 16680, loss: 2215.6631200499833\n",
      "epoch: 0, step: 16690, loss: 2216.899300586432\n",
      "epoch: 0, step: 16700, loss: 2217.9788327924907\n",
      "epoch: 0, step: 16710, loss: 2219.000068832189\n",
      "epoch: 0, step: 16720, loss: 2220.824054379016\n",
      "epoch: 0, step: 16730, loss: 2222.042544428259\n",
      "epoch: 0, step: 16740, loss: 2222.8256204910576\n",
      "epoch: 0, step: 16750, loss: 2224.0456451810896\n",
      "epoch: 0, step: 16760, loss: 2224.905278969556\n",
      "epoch: 0, step: 16770, loss: 2226.00052170828\n",
      "epoch: 0, step: 16780, loss: 2227.1304493434727\n",
      "epoch: 0, step: 16790, loss: 2228.5472378842533\n",
      "epoch: 0, step: 16800, loss: 2230.0470554567873\n",
      "epoch: 0, step: 16810, loss: 2232.638809155673\n",
      "epoch: 0, step: 16820, loss: 2234.0075826086104\n",
      "epoch: 0, step: 16830, loss: 2234.9847523234785\n",
      "epoch: 0, step: 16840, loss: 2236.03508855775\n",
      "epoch: 0, step: 16850, loss: 2237.018327381462\n",
      "epoch: 0, step: 16860, loss: 2238.104746129364\n",
      "epoch: 0, step: 16870, loss: 2239.056958388537\n",
      "epoch: 0, step: 16880, loss: 2239.9479340054095\n",
      "epoch: 0, step: 16890, loss: 2241.226015303284\n",
      "epoch: 0, step: 16900, loss: 2242.7920668013394\n",
      "epoch: 0, step: 16910, loss: 2244.0653357468545\n",
      "epoch: 0, step: 16920, loss: 2245.0172124914825\n",
      "epoch: 0, step: 16930, loss: 2246.038462679833\n",
      "epoch: 0, step: 16940, loss: 2247.020786847919\n",
      "epoch: 0, step: 16950, loss: 2248.3790357373655\n",
      "epoch: 0, step: 16960, loss: 2249.66774629429\n",
      "epoch: 0, step: 16970, loss: 2251.8711118660867\n",
      "epoch: 0, step: 16980, loss: 2253.069323722273\n",
      "epoch: 0, step: 16990, loss: 2253.9984812550247\n",
      "epoch: 0, step: 17000, loss: 2255.465625245124\n",
      "epoch: 0, step: 17010, loss: 2256.3602441214025\n",
      "epoch: 0, step: 17020, loss: 2257.3795326389372\n",
      "epoch: 0, step: 17030, loss: 2258.696419287473\n",
      "epoch: 0, step: 17040, loss: 2259.63557158038\n",
      "epoch: 0, step: 17050, loss: 2260.9211912043393\n",
      "epoch: 0, step: 17060, loss: 2262.0982033871114\n",
      "epoch: 0, step: 17070, loss: 2263.2916317917407\n",
      "epoch: 0, step: 17080, loss: 2264.218643773347\n",
      "epoch: 0, step: 17090, loss: 2265.9743944890797\n",
      "epoch: 0, step: 17100, loss: 2266.88999344036\n",
      "epoch: 0, step: 17110, loss: 2269.555697325617\n",
      "epoch: 0, step: 17120, loss: 2272.566482294351\n",
      "epoch: 0, step: 17130, loss: 2274.3728011883795\n",
      "epoch: 0, step: 17140, loss: 2275.3805094845593\n",
      "epoch: 0, step: 17150, loss: 2276.436652865261\n",
      "epoch: 0, step: 17160, loss: 2277.854357909411\n",
      "epoch: 0, step: 17170, loss: 2279.1380501650274\n",
      "epoch: 0, step: 17180, loss: 2286.5844028554857\n",
      "epoch: 0, step: 17190, loss: 2291.9926857464015\n",
      "epoch: 0, step: 17200, loss: 2297.9779162965715\n",
      "epoch: 0, step: 17210, loss: 2300.884261060506\n",
      "epoch: 0, step: 17220, loss: 2301.9737528674304\n",
      "epoch: 0, step: 17230, loss: 2303.57510221377\n",
      "epoch: 0, step: 17240, loss: 2304.8490271680057\n",
      "epoch: 0, step: 17250, loss: 2306.318577680737\n",
      "epoch: 0, step: 17260, loss: 2307.7452094741166\n",
      "epoch: 0, step: 17270, loss: 2308.8673595376313\n",
      "epoch: 0, step: 17280, loss: 2309.9924576990306\n",
      "epoch: 0, step: 17290, loss: 2311.20060268417\n",
      "epoch: 0, step: 17300, loss: 2312.4687475450337\n",
      "epoch: 0, step: 17310, loss: 2314.3007175438106\n",
      "epoch: 0, step: 17320, loss: 2315.557951372117\n",
      "epoch: 0, step: 17330, loss: 2317.14948098734\n",
      "epoch: 0, step: 17340, loss: 2318.3245705105364\n",
      "epoch: 0, step: 17350, loss: 2319.746599148959\n",
      "epoch: 0, step: 17360, loss: 2320.984918292612\n",
      "epoch: 0, step: 17370, loss: 2322.361871417612\n",
      "epoch: 0, step: 17380, loss: 2323.4058102332056\n",
      "epoch: 0, step: 17390, loss: 2325.6829825155437\n",
      "epoch: 0, step: 17400, loss: 2330.9745466820896\n",
      "epoch: 0, step: 17410, loss: 2337.8907314650714\n",
      "epoch: 0, step: 17420, loss: 2343.442964669317\n",
      "epoch: 0, step: 17430, loss: 2345.3018845655024\n",
      "epoch: 0, step: 17440, loss: 2346.406682368368\n",
      "epoch: 0, step: 17450, loss: 2347.8311085663736\n",
      "epoch: 0, step: 17460, loss: 2348.8553692512214\n",
      "epoch: 0, step: 17470, loss: 2349.990573581308\n",
      "epoch: 0, step: 17480, loss: 2351.3445986919105\n",
      "epoch: 0, step: 17490, loss: 2352.775970686227\n",
      "epoch: 0, step: 17500, loss: 2353.944668058306\n",
      "epoch: 0, step: 17510, loss: 2355.204589713365\n",
      "epoch: 0, step: 17520, loss: 2356.3219362460077\n",
      "epoch: 0, step: 17530, loss: 2357.415121924132\n",
      "epoch: 0, step: 17540, loss: 2358.3996016792953\n",
      "epoch: 0, step: 17550, loss: 2359.77945015952\n",
      "epoch: 0, step: 17560, loss: 2362.269189301878\n",
      "epoch: 0, step: 17570, loss: 2363.512871373445\n",
      "epoch: 0, step: 17580, loss: 2364.7426641993225\n",
      "epoch: 0, step: 17590, loss: 2365.936343308538\n",
      "epoch: 0, step: 17600, loss: 2366.9532483257353\n",
      "epoch: 0, step: 17610, loss: 2368.133596751839\n",
      "epoch: 0, step: 17620, loss: 2369.096302319318\n",
      "epoch: 0, step: 17630, loss: 2370.057208571583\n",
      "epoch: 0, step: 17640, loss: 2371.339646089822\n",
      "epoch: 0, step: 17650, loss: 2372.2760109864175\n",
      "epoch: 0, step: 17660, loss: 2373.3684432171285\n",
      "epoch: 0, step: 17670, loss: 2374.400325689465\n",
      "epoch: 0, step: 17680, loss: 2375.73793431744\n",
      "epoch: 0, step: 17690, loss: 2377.90300135687\n",
      "epoch: 0, step: 17700, loss: 2379.4953074119985\n",
      "epoch: 0, step: 17710, loss: 2380.2726505734026\n",
      "epoch: 0, step: 17720, loss: 2381.5656146146357\n",
      "epoch: 0, step: 17730, loss: 2382.816899921745\n",
      "epoch: 0, step: 17740, loss: 2384.298734229058\n",
      "epoch: 0, step: 17750, loss: 2385.633601065725\n",
      "epoch: 0, step: 17760, loss: 2386.7331244312227\n",
      "epoch: 0, step: 17770, loss: 2388.1101435236633\n",
      "epoch: 0, step: 17780, loss: 2389.041296634823\n",
      "epoch: 0, step: 17790, loss: 2390.2313324548304\n",
      "epoch: 0, step: 17800, loss: 2391.331924472004\n",
      "epoch: 0, step: 17810, loss: 2392.5608349330723\n",
      "epoch: 0, step: 17820, loss: 2393.792219053954\n",
      "epoch: 0, step: 17830, loss: 2395.035677548498\n",
      "epoch: 0, step: 17840, loss: 2396.1265137530863\n",
      "epoch: 0, step: 17850, loss: 2397.265428531915\n",
      "epoch: 0, step: 17860, loss: 2398.342565204948\n",
      "epoch: 0, step: 17870, loss: 2399.890798319131\n",
      "epoch: 0, step: 17880, loss: 2400.9997323341668\n",
      "epoch: 0, step: 17890, loss: 2402.282464515418\n",
      "epoch: 0, step: 17900, loss: 2404.807377677411\n",
      "epoch: 0, step: 17910, loss: 2406.325477745384\n",
      "epoch: 0, step: 17920, loss: 2407.5160005502403\n",
      "epoch: 0, step: 17930, loss: 2408.9866587258875\n",
      "epoch: 0, step: 17940, loss: 2409.9586130790412\n",
      "epoch: 0, step: 17950, loss: 2410.885090176016\n",
      "epoch: 0, step: 17960, loss: 2412.0120684169233\n",
      "epoch: 0, step: 17970, loss: 2413.4167846627533\n",
      "epoch: 0, step: 17980, loss: 2414.544264767319\n",
      "epoch: 0, step: 17990, loss: 2415.748772304505\n",
      "epoch: 0, step: 18000, loss: 2416.873640898615\n",
      "epoch: 0, step: 18010, loss: 2417.938797120005\n",
      "epoch: 0, step: 18020, loss: 2419.0111731626093\n",
      "epoch: 0, step: 18030, loss: 2420.3378599248827\n",
      "epoch: 0, step: 18040, loss: 2421.671863924712\n",
      "epoch: 0, step: 18050, loss: 2424.9390468113124\n",
      "epoch: 0, step: 18060, loss: 2427.0757235251367\n",
      "epoch: 0, step: 18070, loss: 2428.2467000596225\n",
      "epoch: 0, step: 18080, loss: 2429.25626892969\n",
      "epoch: 0, step: 18090, loss: 2430.120223734528\n",
      "epoch: 0, step: 18100, loss: 2431.2987201474607\n",
      "epoch: 0, step: 18110, loss: 2432.5162230096757\n",
      "epoch: 0, step: 18120, loss: 2434.084041584283\n",
      "epoch: 0, step: 18130, loss: 2435.2315832711756\n",
      "epoch: 0, step: 18140, loss: 2436.4112403281033\n",
      "epoch: 0, step: 18150, loss: 2437.463244769722\n",
      "epoch: 0, step: 18160, loss: 2438.4870344661176\n",
      "epoch: 0, step: 18170, loss: 2439.704923477024\n",
      "epoch: 0, step: 18180, loss: 2441.0146680139005\n",
      "epoch: 0, step: 18190, loss: 2442.053820747882\n",
      "epoch: 0, step: 18200, loss: 2443.0731568671763\n",
      "epoch: 0, step: 18210, loss: 2444.2655225358903\n",
      "epoch: 0, step: 18220, loss: 2445.1915641240776\n",
      "epoch: 0, step: 18230, loss: 2446.587960910052\n",
      "epoch: 0, step: 18240, loss: 2447.71352025494\n",
      "epoch: 0, step: 18250, loss: 2449.511992070824\n",
      "epoch: 0, step: 18260, loss: 2451.0120407156646\n",
      "epoch: 0, step: 18270, loss: 2452.349134553224\n",
      "epoch: 0, step: 18280, loss: 2453.607540111989\n",
      "epoch: 0, step: 18290, loss: 2454.80814429\n",
      "epoch: 0, step: 18300, loss: 2455.8415882848203\n",
      "epoch: 0, step: 18310, loss: 2457.5642058961093\n",
      "epoch: 0, step: 18320, loss: 2458.6738238297403\n",
      "epoch: 0, step: 18330, loss: 2459.8263790868223\n",
      "epoch: 0, step: 18340, loss: 2460.781897749752\n",
      "epoch: 0, step: 18350, loss: 2461.862800400704\n",
      "epoch: 0, step: 18360, loss: 2462.822009842843\n",
      "epoch: 0, step: 18370, loss: 2463.642599146813\n",
      "epoch: 0, step: 18380, loss: 2465.2946810536087\n",
      "epoch: 0, step: 18390, loss: 2466.242632318288\n",
      "epoch: 0, step: 18400, loss: 2467.210448075086\n",
      "epoch: 0, step: 18410, loss: 2468.3195563219488\n",
      "epoch: 0, step: 18420, loss: 2469.599302534014\n",
      "epoch: 0, step: 18430, loss: 2471.344676371664\n",
      "epoch: 0, step: 18440, loss: 2472.674465890974\n",
      "epoch: 0, step: 18450, loss: 2474.023512799293\n",
      "epoch: 0, step: 18460, loss: 2475.3458374477923\n",
      "epoch: 0, step: 18470, loss: 2476.4319847486913\n",
      "epoch: 0, step: 18480, loss: 2477.906111676246\n",
      "epoch: 0, step: 18490, loss: 2478.8592508621514\n",
      "epoch: 0, step: 18500, loss: 2479.941232379526\n",
      "epoch: 0, step: 18510, loss: 2480.8876424841583\n",
      "epoch: 0, step: 18520, loss: 2481.920805539936\n",
      "epoch: 0, step: 18530, loss: 2482.9971967451274\n",
      "epoch: 0, step: 18540, loss: 2483.8461940698326\n",
      "epoch: 0, step: 18550, loss: 2485.0685821436346\n",
      "epoch: 0, step: 18560, loss: 2486.329004663974\n",
      "epoch: 0, step: 18570, loss: 2487.2962951771915\n",
      "epoch: 0, step: 18580, loss: 2488.2820475436747\n",
      "epoch: 0, step: 18590, loss: 2489.432379003614\n",
      "epoch: 0, step: 18600, loss: 2490.474779460579\n",
      "epoch: 0, step: 18610, loss: 2491.8539861552417\n",
      "epoch: 0, step: 18620, loss: 2492.9519765116274\n",
      "epoch: 0, step: 18630, loss: 2494.0218592323363\n",
      "epoch: 0, step: 18640, loss: 2495.854239549488\n",
      "epoch: 0, step: 18650, loss: 2497.350650180131\n",
      "epoch: 0, step: 18660, loss: 2499.47549142316\n",
      "epoch: 0, step: 18670, loss: 2501.9487808980048\n",
      "epoch: 0, step: 18680, loss: 2502.697550650686\n",
      "epoch: 0, step: 18690, loss: 2503.5842594094574\n",
      "epoch: 0, step: 18700, loss: 2504.573953267187\n",
      "epoch: 0, step: 18710, loss: 2505.321183886379\n",
      "epoch: 0, step: 18720, loss: 2506.291405584663\n",
      "epoch: 0, step: 18730, loss: 2507.0976941064\n",
      "epoch: 0, step: 18740, loss: 2508.051070868969\n",
      "epoch: 0, step: 18750, loss: 2509.0827070474625\n",
      "epoch: 0, step: 18760, loss: 2510.0739539936185\n",
      "epoch: 0, step: 18770, loss: 2511.000585965812\n",
      "epoch: 0, step: 18780, loss: 2512.252256922424\n",
      "epoch: 0, step: 18790, loss: 2513.363960534334\n",
      "epoch: 0, step: 18800, loss: 2515.9593475311995\n",
      "epoch: 0, step: 18810, loss: 2522.052441224456\n",
      "epoch: 0, step: 18820, loss: 2528.1644004136324\n",
      "epoch: 0, step: 18830, loss: 2531.7028612941504\n",
      "epoch: 0, step: 18840, loss: 2534.937573082745\n",
      "epoch: 0, step: 18850, loss: 2537.7959809079766\n",
      "epoch: 0, step: 18860, loss: 2538.9526724368334\n",
      "epoch: 0, step: 18870, loss: 2539.889564178884\n",
      "epoch: 0, step: 18880, loss: 2541.1432081460953\n",
      "epoch: 0, step: 18890, loss: 2542.1601371094584\n",
      "epoch: 0, step: 18900, loss: 2543.2543859407306\n",
      "epoch: 0, step: 18910, loss: 2544.2643092870712\n",
      "epoch: 0, step: 18920, loss: 2545.520954273641\n",
      "epoch: 0, step: 18930, loss: 2547.763458713889\n",
      "epoch: 0, step: 18940, loss: 2549.734611965716\n",
      "epoch: 0, step: 18950, loss: 2550.886994585395\n",
      "epoch: 0, step: 18960, loss: 2551.8057855069637\n",
      "epoch: 0, step: 18970, loss: 2553.0431366562843\n",
      "epoch: 0, step: 18980, loss: 2553.987402662635\n",
      "epoch: 0, step: 18990, loss: 2554.944923058152\n",
      "epoch: 0, step: 19000, loss: 2556.228854343295\n",
      "epoch: 0, step: 19010, loss: 2557.0952347293496\n",
      "epoch: 0, step: 19020, loss: 2558.9002372846007\n",
      "epoch: 0, step: 19030, loss: 2559.9027472510934\n",
      "epoch: 0, step: 19040, loss: 2561.0353224799037\n",
      "epoch: 0, step: 19050, loss: 2562.461919605732\n",
      "epoch: 0, step: 19060, loss: 2563.6678007766604\n",
      "epoch: 0, step: 19070, loss: 2564.9718780443072\n",
      "epoch: 0, step: 19080, loss: 2565.946090579033\n",
      "epoch: 0, step: 19090, loss: 2567.134382069111\n",
      "epoch: 0, step: 19100, loss: 2568.0775192677975\n",
      "epoch: 0, step: 19110, loss: 2569.159840375185\n",
      "epoch: 0, step: 19120, loss: 2570.075957119465\n",
      "epoch: 0, step: 19130, loss: 2571.0782364308834\n",
      "epoch: 0, step: 19140, loss: 2572.3548253253102\n",
      "epoch: 0, step: 19150, loss: 2573.467432014644\n",
      "epoch: 0, step: 19160, loss: 2575.00462449342\n",
      "epoch: 0, step: 19170, loss: 2576.1119378060102\n",
      "epoch: 0, step: 19180, loss: 2577.2175205647945\n",
      "epoch: 0, step: 19190, loss: 2578.375052988529\n",
      "epoch: 0, step: 19200, loss: 2579.5124591365457\n",
      "epoch: 0, step: 19210, loss: 2580.833512634039\n",
      "epoch: 0, step: 19220, loss: 2581.993169359863\n",
      "epoch: 0, step: 19230, loss: 2582.8960250020027\n",
      "epoch: 0, step: 19240, loss: 2583.883672878146\n",
      "epoch: 0, step: 19250, loss: 2584.973819360137\n",
      "epoch: 0, step: 19260, loss: 2586.285649575293\n",
      "epoch: 0, step: 19270, loss: 2587.4876237362623\n",
      "epoch: 0, step: 19280, loss: 2588.4914064854383\n",
      "epoch: 0, step: 19290, loss: 2589.885354526341\n",
      "epoch: 0, step: 19300, loss: 2590.9904079288244\n",
      "epoch: 0, step: 19310, loss: 2591.8219692632556\n",
      "epoch: 0, step: 19320, loss: 2593.221965841949\n",
      "epoch: 0, step: 19330, loss: 2594.8010061085224\n",
      "epoch: 0, step: 19340, loss: 2596.4443254843354\n",
      "epoch: 0, step: 19350, loss: 2597.7409378290176\n",
      "epoch: 0, step: 19360, loss: 2598.7715430259705\n",
      "epoch: 0, step: 19370, loss: 2600.2178974226117\n",
      "epoch: 0, step: 19380, loss: 2601.5326722487807\n",
      "epoch: 0, step: 19390, loss: 2602.8940590545535\n",
      "epoch: 0, step: 19400, loss: 2604.126567043364\n",
      "epoch: 0, step: 19410, loss: 2605.266317613423\n",
      "epoch: 0, step: 19420, loss: 2606.3476655632257\n",
      "epoch: 0, step: 19430, loss: 2607.5088529735804\n",
      "epoch: 0, step: 19440, loss: 2608.6881857663393\n",
      "epoch: 0, step: 19450, loss: 2609.649029288441\n",
      "epoch: 0, step: 19460, loss: 2610.8158579953015\n",
      "epoch: 0, step: 19470, loss: 2612.2808723263443\n",
      "epoch: 0, step: 19480, loss: 2613.4965083561838\n",
      "epoch: 0, step: 19490, loss: 2614.7309111319482\n",
      "epoch: 0, step: 19500, loss: 2615.771605219692\n",
      "epoch: 0, step: 19510, loss: 2616.999025527388\n",
      "epoch: 0, step: 19520, loss: 2617.7926861084998\n",
      "epoch: 0, step: 19530, loss: 2618.9229463897645\n",
      "epoch: 0, step: 19540, loss: 2619.896254081279\n",
      "epoch: 0, step: 19550, loss: 2621.4895072691143\n",
      "epoch: 0, step: 19560, loss: 2622.919085573405\n",
      "epoch: 0, step: 19570, loss: 2625.1423793546855\n",
      "epoch: 0, step: 19580, loss: 2626.453747909516\n",
      "epoch: 0, step: 19590, loss: 2628.2223262749612\n",
      "epoch: 0, step: 19600, loss: 2629.462860416621\n",
      "epoch: 0, step: 19610, loss: 2631.151740003377\n",
      "epoch: 0, step: 19620, loss: 2633.726275611669\n",
      "epoch: 0, step: 19630, loss: 2634.8805432207882\n",
      "epoch: 0, step: 19640, loss: 2636.144591804594\n",
      "epoch: 0, step: 19650, loss: 2637.003977213055\n",
      "epoch: 0, step: 19660, loss: 2638.0280947722495\n",
      "epoch: 0, step: 19670, loss: 2639.1559267379344\n",
      "epoch: 0, step: 19680, loss: 2640.4325113780797\n",
      "epoch: 0, step: 19690, loss: 2643.0958058200777\n",
      "epoch: 0, step: 19700, loss: 2646.078313972801\n",
      "epoch: 0, step: 19710, loss: 2647.186194714159\n",
      "epoch: 0, step: 19720, loss: 2648.0526355691254\n",
      "epoch: 0, step: 19730, loss: 2648.9721536673605\n",
      "epoch: 0, step: 19740, loss: 2650.045966949314\n",
      "epoch: 0, step: 19750, loss: 2650.970436681062\n",
      "epoch: 0, step: 19760, loss: 2651.965073276311\n",
      "epoch: 0, step: 19770, loss: 2653.186653409153\n",
      "epoch: 0, step: 19780, loss: 2654.830148074776\n",
      "epoch: 0, step: 19790, loss: 2656.6851356886327\n",
      "epoch: 0, step: 19800, loss: 2657.6039709188044\n",
      "epoch: 0, step: 19810, loss: 2660.676697935909\n",
      "epoch: 0, step: 19820, loss: 2663.0412030629814\n",
      "epoch: 0, step: 19830, loss: 2663.901428747922\n",
      "epoch: 0, step: 19840, loss: 2665.023048106581\n",
      "epoch: 0, step: 19850, loss: 2666.1159753613174\n",
      "epoch: 0, step: 19860, loss: 2667.0433380566537\n",
      "epoch: 0, step: 19870, loss: 2668.0105784349144\n",
      "epoch: 0, step: 19880, loss: 2669.470727171749\n",
      "epoch: 0, step: 19890, loss: 2671.8314622975886\n",
      "epoch: 0, step: 19900, loss: 2672.872407760471\n",
      "epoch: 0, step: 19910, loss: 2675.1767385862768\n",
      "epoch: 0, step: 19920, loss: 2676.157421629876\n",
      "epoch: 0, step: 19930, loss: 2677.146067548543\n",
      "epoch: 0, step: 19940, loss: 2678.123001907021\n",
      "epoch: 0, step: 19950, loss: 2678.9326515607536\n",
      "epoch: 0, step: 19960, loss: 2679.6890847198665\n",
      "epoch: 0, step: 19970, loss: 2681.0047976709902\n",
      "epoch: 0, step: 19980, loss: 2682.1914249770343\n",
      "epoch: 0, step: 19990, loss: 2684.72155797109\n",
      "epoch: 0, step: 20000, loss: 2686.3214548788965\n",
      "epoch: 0, step: 20010, loss: 2688.171714205295\n",
      "epoch: 0, step: 20020, loss: 2689.3248654194176\n",
      "epoch: 0, step: 20030, loss: 2690.590434078127\n",
      "epoch: 0, step: 20040, loss: 2691.546664763242\n",
      "epoch: 0, step: 20050, loss: 2692.7999397180974\n",
      "epoch: 0, step: 20060, loss: 2693.691335681826\n",
      "epoch: 0, step: 20070, loss: 2694.547654841095\n",
      "epoch: 0, step: 20080, loss: 2695.9738485850394\n",
      "epoch: 0, step: 20090, loss: 2697.067896153778\n",
      "epoch: 0, step: 20100, loss: 2698.863304641098\n",
      "epoch: 0, step: 20110, loss: 2700.4134079031646\n",
      "epoch: 0, step: 20120, loss: 2701.7618026025593\n",
      "epoch: 0, step: 20130, loss: 2703.081449780613\n",
      "epoch: 0, step: 20140, loss: 2704.4015627987683\n",
      "epoch: 0, step: 20150, loss: 2705.473855096847\n",
      "epoch: 0, step: 20160, loss: 2706.652953054756\n",
      "epoch: 0, step: 20170, loss: 2707.6123376078904\n",
      "epoch: 0, step: 20180, loss: 2708.6676367260516\n",
      "epoch: 0, step: 20190, loss: 2709.735059451312\n",
      "epoch: 0, step: 20200, loss: 2710.949356917292\n",
      "epoch: 0, step: 20210, loss: 2711.85376502946\n",
      "epoch: 0, step: 20220, loss: 2713.1155983023345\n",
      "epoch: 0, step: 20230, loss: 2714.0243643783033\n",
      "epoch: 0, step: 20240, loss: 2715.0070244260132\n",
      "epoch: 0, step: 20250, loss: 2716.925874758512\n",
      "epoch: 0, step: 20260, loss: 2720.3313057757914\n",
      "epoch: 0, step: 20270, loss: 2723.485740620643\n",
      "epoch: 0, step: 20280, loss: 2725.7381921447814\n",
      "epoch: 0, step: 20290, loss: 2726.9366430081427\n",
      "epoch: 0, step: 20300, loss: 2727.939990568906\n",
      "epoch: 0, step: 20310, loss: 2728.8323753736913\n",
      "epoch: 0, step: 20320, loss: 2729.834559712559\n",
      "epoch: 0, step: 20330, loss: 2731.149066131562\n",
      "epoch: 0, step: 20340, loss: 2732.137200575322\n",
      "epoch: 0, step: 20350, loss: 2733.2346240244806\n",
      "epoch: 0, step: 20360, loss: 2734.5425768233836\n",
      "epoch: 0, step: 20370, loss: 2735.430565107614\n",
      "epoch: 0, step: 20380, loss: 2736.538113567978\n",
      "epoch: 0, step: 20390, loss: 2737.4905378110707\n",
      "epoch: 0, step: 20400, loss: 2738.718182388693\n",
      "epoch: 0, step: 20410, loss: 2739.7062249816954\n",
      "epoch: 0, step: 20420, loss: 2741.6803592182696\n",
      "epoch: 0, step: 20430, loss: 2742.7210767008364\n",
      "epoch: 0, step: 20440, loss: 2744.1219074241817\n",
      "epoch: 0, step: 20450, loss: 2745.218477834016\n",
      "epoch: 0, step: 20460, loss: 2746.4254851303995\n",
      "epoch: 0, step: 20470, loss: 2747.4491296596825\n",
      "epoch: 0, step: 20480, loss: 2748.951859023422\n",
      "epoch: 0, step: 20490, loss: 2750.638864185661\n",
      "epoch: 0, step: 20500, loss: 2751.940962333232\n",
      "epoch: 0, step: 20510, loss: 2753.217974048108\n",
      "epoch: 0, step: 20520, loss: 2754.154530134052\n",
      "epoch: 0, step: 20530, loss: 2754.9688321985304\n",
      "epoch: 0, step: 20540, loss: 2756.1524103693664\n",
      "epoch: 0, step: 20550, loss: 2757.2780147604644\n",
      "epoch: 0, step: 20560, loss: 2758.327009689063\n",
      "epoch: 0, step: 20570, loss: 2759.5450984723866\n",
      "epoch: 0, step: 20580, loss: 2760.907138746232\n",
      "epoch: 0, step: 20590, loss: 2761.8644065372646\n",
      "epoch: 0, step: 20600, loss: 2762.9684302695096\n",
      "epoch: 0, step: 20610, loss: 2768.1008651144803\n",
      "epoch: 0, step: 20620, loss: 2769.3886364661157\n",
      "epoch: 0, step: 20630, loss: 2770.649361964315\n",
      "epoch: 0, step: 20640, loss: 2771.7861615158617\n",
      "epoch: 0, step: 20650, loss: 2773.3133910112083\n",
      "epoch: 0, step: 20660, loss: 2777.536702450365\n",
      "epoch: 0, step: 20670, loss: 2780.8987571261823\n",
      "epoch: 0, step: 20680, loss: 2782.42904220894\n",
      "epoch: 0, step: 20690, loss: 2783.4722320474684\n",
      "epoch: 0, step: 20700, loss: 2784.9047949425876\n",
      "epoch: 0, step: 20710, loss: 2785.7836925424635\n",
      "epoch: 0, step: 20720, loss: 2786.9421078749\n",
      "epoch: 0, step: 20730, loss: 2787.8630292229354\n",
      "epoch: 0, step: 20740, loss: 2789.0297312252223\n",
      "epoch: 0, step: 20750, loss: 2789.9836543761194\n",
      "epoch: 0, step: 20760, loss: 2795.371812786907\n",
      "epoch: 0, step: 20770, loss: 2796.7730149812996\n",
      "epoch: 0, step: 20780, loss: 2798.0528256334364\n",
      "epoch: 0, step: 20790, loss: 2799.1074712984264\n",
      "epoch: 0, step: 20800, loss: 2800.3448721282184\n",
      "epoch: 0, step: 20810, loss: 2801.721463058144\n",
      "epoch: 0, step: 20820, loss: 2803.700888272375\n",
      "epoch: 0, step: 20830, loss: 2805.046430002898\n",
      "epoch: 0, step: 20840, loss: 2806.429288137704\n",
      "epoch: 0, step: 20850, loss: 2808.040981564671\n",
      "epoch: 0, step: 20860, loss: 2809.052376303822\n",
      "epoch: 0, step: 20870, loss: 2810.0372722931206\n",
      "epoch: 0, step: 20880, loss: 2812.5092846639454\n",
      "epoch: 0, step: 20890, loss: 2813.9968875087798\n",
      "epoch: 0, step: 20900, loss: 2814.9004901386797\n",
      "epoch: 0, step: 20910, loss: 2815.755140390247\n",
      "epoch: 0, step: 20920, loss: 2816.8871617652476\n",
      "epoch: 0, step: 20930, loss: 2818.260372657329\n",
      "epoch: 0, step: 20940, loss: 2819.537527192384\n",
      "epoch: 0, step: 20950, loss: 2820.949475776404\n",
      "epoch: 0, step: 20960, loss: 2822.0008834935725\n",
      "epoch: 0, step: 20970, loss: 2822.8924983255565\n",
      "epoch: 0, step: 20980, loss: 2824.041067700833\n",
      "epoch: 0, step: 20990, loss: 2824.991129692644\n",
      "epoch: 0, step: 21000, loss: 2826.483862850815\n",
      "epoch: 0, step: 21010, loss: 2827.6059698052704\n",
      "epoch: 0, step: 21020, loss: 2828.6839899756014\n",
      "epoch: 0, step: 21030, loss: 2829.7458460517228\n",
      "epoch: 0, step: 21040, loss: 2830.797123271972\n",
      "epoch: 0, step: 21050, loss: 2831.758828740567\n",
      "epoch: 0, step: 21060, loss: 2833.293411295861\n",
      "epoch: 0, step: 21070, loss: 2834.400529164821\n",
      "epoch: 0, step: 21080, loss: 2835.483081433922\n",
      "epoch: 0, step: 21090, loss: 2836.714905563742\n",
      "epoch: 0, step: 21100, loss: 2837.714936900884\n",
      "epoch: 0, step: 21110, loss: 2838.8683341555297\n",
      "epoch: 0, step: 21120, loss: 2839.7451866827905\n",
      "epoch: 0, step: 21130, loss: 2840.5414819680154\n",
      "epoch: 0, step: 21140, loss: 2841.5716504491866\n",
      "epoch: 0, step: 21150, loss: 2842.934224497527\n",
      "epoch: 0, step: 21160, loss: 2843.8961736746132\n",
      "epoch: 0, step: 21170, loss: 2844.773685786873\n",
      "epoch: 0, step: 21180, loss: 2846.0263587646186\n",
      "epoch: 0, step: 21190, loss: 2847.003180872649\n",
      "epoch: 0, step: 21200, loss: 2848.8563653342426\n",
      "epoch: 0, step: 21210, loss: 2850.0912851057947\n",
      "epoch: 0, step: 21220, loss: 2852.390815805644\n",
      "epoch: 0, step: 21230, loss: 2854.8058702759445\n",
      "epoch: 0, step: 21240, loss: 2857.043888848275\n",
      "epoch: 0, step: 21250, loss: 2858.0683045201004\n",
      "epoch: 0, step: 21260, loss: 2858.964954394847\n",
      "epoch: 0, step: 21270, loss: 2859.899420786649\n",
      "epoch: 0, step: 21280, loss: 2861.0078707374632\n",
      "epoch: 0, step: 21290, loss: 2862.3095868118107\n",
      "epoch: 0, step: 21300, loss: 2863.321636211127\n",
      "epoch: 0, step: 21310, loss: 2864.90872650221\n",
      "epoch: 0, step: 21320, loss: 2867.5062422417104\n",
      "epoch: 0, step: 21330, loss: 2870.0142243914306\n",
      "epoch: 0, step: 21340, loss: 2870.9930374510586\n",
      "epoch: 0, step: 21350, loss: 2871.9978527985513\n",
      "epoch: 0, step: 21360, loss: 2874.716384228319\n",
      "epoch: 0, step: 21370, loss: 2876.016908098012\n",
      "epoch: 0, step: 21380, loss: 2877.7709304876626\n",
      "epoch: 0, step: 21390, loss: 2879.027085084468\n",
      "epoch: 0, step: 21400, loss: 2880.297137055546\n",
      "epoch: 0, step: 21410, loss: 2881.322877597064\n",
      "epoch: 0, step: 21420, loss: 2883.1866745240986\n",
      "epoch: 0, step: 21430, loss: 2884.3074867911637\n",
      "epoch: 0, step: 21440, loss: 2885.3453631289303\n",
      "epoch: 0, step: 21450, loss: 2886.679699946195\n",
      "epoch: 0, step: 21460, loss: 2888.0928932465613\n",
      "epoch: 0, step: 21470, loss: 2889.1109254322946\n",
      "epoch: 0, step: 21480, loss: 2890.739579964429\n",
      "epoch: 0, step: 21490, loss: 2892.109934080392\n",
      "epoch: 0, step: 21500, loss: 2893.202438350767\n",
      "epoch: 0, step: 21510, loss: 2894.8512670733035\n",
      "epoch: 0, step: 21520, loss: 2896.261443231255\n",
      "epoch: 0, step: 21530, loss: 2897.4994911141694\n",
      "epoch: 0, step: 21540, loss: 2898.862323652953\n",
      "epoch: 0, step: 21550, loss: 2899.828635957092\n",
      "epoch: 0, step: 21560, loss: 2900.921146746725\n",
      "epoch: 0, step: 21570, loss: 2901.9456348009408\n",
      "epoch: 0, step: 21580, loss: 2903.1237854622304\n",
      "epoch: 0, step: 21590, loss: 2904.0701561681926\n",
      "epoch: 0, step: 21600, loss: 2905.071339663118\n",
      "epoch: 0, step: 21610, loss: 2906.4548134170473\n",
      "epoch: 0, step: 21620, loss: 2907.8206607215106\n",
      "epoch: 0, step: 21630, loss: 2909.0725536309183\n",
      "epoch: 0, step: 21640, loss: 2910.2311372719705\n",
      "epoch: 0, step: 21650, loss: 2911.582413304597\n",
      "epoch: 0, step: 21660, loss: 2912.9250126965344\n",
      "epoch: 0, step: 21670, loss: 2913.9886195622385\n",
      "epoch: 0, step: 21680, loss: 2915.6305185593665\n",
      "epoch: 0, step: 21690, loss: 2916.5751635096967\n",
      "epoch: 0, step: 21700, loss: 2917.810712080449\n",
      "epoch: 0, step: 21710, loss: 2918.7821401394904\n",
      "epoch: 0, step: 21720, loss: 2919.6337734498084\n",
      "epoch: 0, step: 21730, loss: 2920.5672348849475\n",
      "epoch: 0, step: 21740, loss: 2921.510098595172\n",
      "epoch: 0, step: 21750, loss: 2922.498272392899\n",
      "epoch: 0, step: 21760, loss: 2923.623883906752\n",
      "epoch: 0, step: 21770, loss: 2925.0347088761628\n",
      "epoch: 0, step: 21780, loss: 2925.9503785781562\n",
      "epoch: 0, step: 21790, loss: 2926.8259263671935\n",
      "epoch: 0, step: 21800, loss: 2927.6569701470435\n",
      "epoch: 0, step: 21810, loss: 2928.8593813590705\n",
      "epoch: 0, step: 21820, loss: 2929.787098478526\n",
      "epoch: 0, step: 21830, loss: 2930.756934363395\n",
      "epoch: 0, step: 21840, loss: 2932.1410127319396\n",
      "epoch: 0, step: 21850, loss: 2933.3188271410763\n",
      "epoch: 0, step: 21860, loss: 2934.8814508877695\n",
      "epoch: 0, step: 21870, loss: 2935.7584380693734\n",
      "epoch: 0, step: 21880, loss: 2937.192647796124\n",
      "epoch: 0, step: 21890, loss: 2938.2571822442114\n",
      "epoch: 0, step: 21900, loss: 2939.1675278656185\n",
      "epoch: 0, step: 21910, loss: 2940.1711386032403\n",
      "epoch: 0, step: 21920, loss: 2941.3337282948196\n",
      "epoch: 0, step: 21930, loss: 2942.548620197922\n",
      "epoch: 0, step: 21940, loss: 2943.5331538133323\n",
      "epoch: 0, step: 21950, loss: 2944.8880839310586\n",
      "epoch: 0, step: 21960, loss: 2946.5031815879047\n",
      "epoch: 0, step: 21970, loss: 2947.904321562499\n",
      "epoch: 0, step: 21980, loss: 2949.2760509736836\n",
      "epoch: 0, step: 21990, loss: 2950.7431409843266\n",
      "epoch: 0, step: 22000, loss: 2951.921829428524\n",
      "epoch: 0, step: 22010, loss: 2953.150668602437\n",
      "epoch: 0, step: 22020, loss: 2954.759663861245\n",
      "epoch: 0, step: 22030, loss: 2958.688122075051\n",
      "epoch: 0, step: 22040, loss: 2960.872073750943\n",
      "epoch: 0, step: 22050, loss: 2962.571388144046\n",
      "epoch: 0, step: 22060, loss: 2964.156373973936\n",
      "epoch: 0, step: 22070, loss: 2965.403591338545\n",
      "epoch: 0, step: 22080, loss: 2966.932439316064\n",
      "epoch: 0, step: 22090, loss: 2967.951271507889\n",
      "epoch: 0, step: 22100, loss: 2969.386246096343\n",
      "epoch: 0, step: 22110, loss: 2970.788700837642\n",
      "epoch: 0, step: 22120, loss: 2971.839115668088\n",
      "epoch: 0, step: 22130, loss: 2974.880824159831\n",
      "epoch: 0, step: 22140, loss: 2976.0991328544915\n",
      "epoch: 0, step: 22150, loss: 2977.341095533222\n",
      "epoch: 0, step: 22160, loss: 2979.307522919029\n",
      "epoch: 0, step: 22170, loss: 2981.0359441675246\n",
      "epoch: 0, step: 22180, loss: 2982.4738532565534\n",
      "epoch: 0, step: 22190, loss: 2983.896876502782\n",
      "epoch: 0, step: 22200, loss: 2984.806469101459\n",
      "epoch: 0, step: 22210, loss: 2986.0842049084604\n",
      "epoch: 0, step: 22220, loss: 2987.09596112743\n",
      "epoch: 0, step: 22230, loss: 2988.175773847848\n",
      "epoch: 0, step: 22240, loss: 2989.5497975833714\n",
      "epoch: 0, step: 22250, loss: 2990.8235898055136\n",
      "epoch: 0, step: 22260, loss: 2991.8325122259557\n",
      "epoch: 0, step: 22270, loss: 2992.818031731993\n",
      "epoch: 0, step: 22280, loss: 2993.6511116586626\n",
      "epoch: 0, step: 22290, loss: 2994.9151120446622\n",
      "epoch: 0, step: 22300, loss: 2996.1402830146253\n",
      "epoch: 0, step: 22310, loss: 2997.1389759145677\n",
      "epoch: 0, step: 22320, loss: 2998.0990188084543\n",
      "epoch: 0, step: 22330, loss: 2999.6366898454726\n",
      "epoch: 0, step: 22340, loss: 3000.4956880770624\n",
      "epoch: 0, step: 22350, loss: 3001.623856153339\n",
      "epoch: 0, step: 22360, loss: 3002.442663628608\n",
      "epoch: 0, step: 22370, loss: 3004.312015708536\n",
      "epoch: 0, step: 22380, loss: 3006.213055599481\n",
      "epoch: 0, step: 22390, loss: 3007.3427712656558\n",
      "epoch: 0, step: 22400, loss: 3008.3386635892093\n",
      "epoch: 0, step: 22410, loss: 3009.23100406304\n",
      "epoch: 0, step: 22420, loss: 3011.141343932599\n",
      "epoch: 0, step: 22430, loss: 3012.4795087091625\n",
      "epoch: 0, step: 22440, loss: 3013.5601170770824\n",
      "epoch: 0, step: 22450, loss: 3015.6316865496337\n",
      "epoch: 0, step: 22460, loss: 3016.7756606824696\n",
      "epoch: 0, step: 22470, loss: 3017.790413264185\n",
      "epoch: 0, step: 22480, loss: 3019.380652356893\n",
      "epoch: 0, step: 22490, loss: 3020.3136666826904\n",
      "epoch: 0, step: 22500, loss: 3021.605245951563\n",
      "epoch: 0, step: 22510, loss: 3022.5493845380843\n",
      "epoch: 0, step: 22520, loss: 3023.492206994444\n",
      "epoch: 0, step: 22530, loss: 3024.549272093922\n",
      "epoch: 0, step: 22540, loss: 3025.5019852481782\n",
      "epoch: 0, step: 22550, loss: 3027.458330731839\n",
      "epoch: 0, step: 22560, loss: 3029.034954663366\n",
      "epoch: 0, step: 22570, loss: 3030.3785203434527\n",
      "epoch: 0, step: 22580, loss: 3031.6041891016066\n",
      "epoch: 0, step: 22590, loss: 3033.0536987893283\n",
      "epoch: 0, step: 22600, loss: 3034.048356477171\n",
      "epoch: 0, step: 22610, loss: 3035.0128611288965\n",
      "epoch: 0, step: 22620, loss: 3035.9275025613606\n",
      "epoch: 0, step: 22630, loss: 3037.0759228654206\n",
      "epoch: 0, step: 22640, loss: 3038.08541027084\n",
      "epoch: 0, step: 22650, loss: 3039.3003099672496\n",
      "epoch: 0, step: 22660, loss: 3040.3838303498924\n",
      "epoch: 0, step: 22670, loss: 3041.749105487019\n",
      "epoch: 0, step: 22680, loss: 3042.777287211269\n",
      "epoch: 0, step: 22690, loss: 3044.01114083454\n",
      "epoch: 0, step: 22700, loss: 3044.9754328094423\n",
      "epoch: 0, step: 22710, loss: 3046.2384420745075\n",
      "epoch: 0, step: 22720, loss: 3047.274836566299\n",
      "epoch: 0, step: 22730, loss: 3048.494510639459\n",
      "epoch: 0, step: 22740, loss: 3050.2526649571955\n",
      "epoch: 0, step: 22750, loss: 3052.939154777676\n",
      "epoch: 0, step: 22760, loss: 3054.4182672463357\n",
      "epoch: 0, step: 22770, loss: 3056.203289952129\n",
      "epoch: 0, step: 22780, loss: 3059.8954250104725\n",
      "epoch: 0, step: 22790, loss: 3061.7582401148975\n",
      "epoch: 0, step: 22800, loss: 3063.031106453389\n",
      "epoch: 0, step: 22810, loss: 3065.066441293806\n",
      "epoch: 0, step: 22820, loss: 3066.0873310528696\n",
      "epoch: 0, step: 22830, loss: 3067.1978953815997\n",
      "epoch: 0, step: 22840, loss: 3068.195823881775\n",
      "epoch: 0, step: 22850, loss: 3069.311975415796\n",
      "epoch: 0, step: 22860, loss: 3070.2508448548615\n",
      "epoch: 0, step: 22870, loss: 3071.2320289202034\n",
      "epoch: 0, step: 22880, loss: 3072.2749146856368\n",
      "epoch: 0, step: 22890, loss: 3073.7411541976035\n",
      "epoch: 0, step: 22900, loss: 3074.90514273569\n",
      "epoch: 0, step: 22910, loss: 3075.6901291050017\n",
      "epoch: 0, step: 22920, loss: 3076.621667366475\n",
      "epoch: 0, step: 22930, loss: 3077.7085817195475\n",
      "epoch: 0, step: 22940, loss: 3078.644842531532\n",
      "epoch: 0, step: 22950, loss: 3079.6705417744815\n",
      "epoch: 0, step: 22960, loss: 3081.173054676503\n",
      "epoch: 0, step: 22970, loss: 3082.510428328067\n",
      "epoch: 0, step: 22980, loss: 3083.956651363522\n",
      "epoch: 0, step: 22990, loss: 3086.1127516813576\n",
      "epoch: 0, step: 23000, loss: 3087.925434719771\n",
      "epoch: 0, step: 23010, loss: 3089.22682986781\n",
      "epoch: 0, step: 23020, loss: 3090.4260918460786\n",
      "epoch: 0, step: 23030, loss: 3091.664361272007\n",
      "epoch: 0, step: 23040, loss: 3092.806375782937\n",
      "epoch: 0, step: 23050, loss: 3093.815707001835\n",
      "epoch: 0, step: 23060, loss: 3094.790734972805\n",
      "epoch: 0, step: 23070, loss: 3095.858366277069\n",
      "epoch: 0, step: 23080, loss: 3096.6703217439353\n",
      "epoch: 0, step: 23090, loss: 3097.7414045371115\n",
      "epoch: 0, step: 23100, loss: 3098.646359626204\n",
      "epoch: 0, step: 23110, loss: 3099.5638365857303\n",
      "epoch: 0, step: 23120, loss: 3100.5181536115706\n",
      "epoch: 0, step: 23130, loss: 3101.357783962041\n",
      "epoch: 0, step: 23140, loss: 3103.129095491022\n",
      "epoch: 0, step: 23150, loss: 3104.473232615739\n",
      "epoch: 0, step: 23160, loss: 3105.5580843426287\n",
      "epoch: 0, step: 23170, loss: 3106.603532347828\n",
      "epoch: 0, step: 23180, loss: 3108.0304069258273\n",
      "epoch: 0, step: 23190, loss: 3109.8921476863325\n",
      "epoch: 0, step: 23200, loss: 3111.2665094546974\n",
      "epoch: 0, step: 23210, loss: 3112.44162177667\n",
      "epoch: 0, step: 23220, loss: 3113.2772768698633\n",
      "epoch: 0, step: 23230, loss: 3114.3427114523947\n",
      "epoch: 0, step: 23240, loss: 3115.385201524943\n",
      "epoch: 0, step: 23250, loss: 3116.5411693267524\n",
      "epoch: 0, step: 23260, loss: 3118.008358914405\n",
      "epoch: 0, step: 23270, loss: 3119.058952059597\n",
      "epoch: 0, step: 23280, loss: 3120.159766871482\n",
      "epoch: 0, step: 23290, loss: 3121.629309851676\n",
      "epoch: 0, step: 23300, loss: 3122.8877987526357\n",
      "epoch: 0, step: 23310, loss: 3124.1578268520534\n",
      "epoch: 0, step: 23320, loss: 3125.151442255825\n",
      "epoch: 0, step: 23330, loss: 3126.321330640465\n",
      "epoch: 0, step: 23340, loss: 3127.2646707855165\n",
      "epoch: 0, step: 23350, loss: 3128.4234894774854\n",
      "epoch: 0, step: 23360, loss: 3129.526248615235\n",
      "epoch: 0, step: 23370, loss: 3130.584072623402\n",
      "epoch: 0, step: 23380, loss: 3132.1986435092986\n",
      "epoch: 0, step: 23390, loss: 3133.133430156857\n",
      "epoch: 0, step: 23400, loss: 3134.0752934627235\n",
      "epoch: 0, step: 23410, loss: 3135.271214108914\n",
      "epoch: 0, step: 23420, loss: 3136.7765861414373\n",
      "epoch: 0, step: 23430, loss: 3138.783304620534\n",
      "epoch: 0, step: 23440, loss: 3140.9216205216944\n",
      "epoch: 0, step: 23450, loss: 3142.7470952011645\n",
      "epoch: 0, step: 23460, loss: 3144.100805852562\n",
      "epoch: 0, step: 23470, loss: 3145.6211072020233\n",
      "epoch: 0, step: 23480, loss: 3146.6935499198735\n",
      "epoch: 0, step: 23490, loss: 3148.493301767856\n",
      "epoch: 0, step: 23500, loss: 3149.6944148130715\n",
      "epoch: 0, step: 23510, loss: 3150.883305784315\n",
      "epoch: 0, step: 23520, loss: 3151.792053427547\n",
      "epoch: 0, step: 23530, loss: 3153.0323987193406\n",
      "epoch: 0, step: 23540, loss: 3154.332334201783\n",
      "epoch: 0, step: 23550, loss: 3155.5607993118465\n",
      "epoch: 0, step: 23560, loss: 3156.62033848837\n",
      "epoch: 0, step: 23570, loss: 3157.701780643314\n",
      "epoch: 0, step: 23580, loss: 3158.7428591512144\n",
      "epoch: 0, step: 23590, loss: 3159.670188244432\n",
      "epoch: 0, step: 23600, loss: 3160.8239788748324\n",
      "epoch: 0, step: 23610, loss: 3161.8661252819\n",
      "epoch: 0, step: 23620, loss: 3162.79339742288\n",
      "epoch: 0, step: 23630, loss: 3163.806485991925\n",
      "epoch: 0, step: 23640, loss: 3164.9481515549123\n",
      "epoch: 0, step: 23650, loss: 3165.9383928589523\n",
      "epoch: 0, step: 23660, loss: 3167.8009007312357\n",
      "epoch: 0, step: 23670, loss: 3169.1802283264697\n",
      "epoch: 0, step: 23680, loss: 3170.715306226164\n",
      "epoch: 0, step: 23690, loss: 3171.881101038307\n",
      "epoch: 0, step: 23700, loss: 3173.115914899856\n",
      "epoch: 0, step: 23710, loss: 3174.0723361186683\n",
      "epoch: 0, step: 23720, loss: 3174.9983836077154\n",
      "epoch: 0, step: 23730, loss: 3175.8189088366926\n",
      "epoch: 0, step: 23740, loss: 3176.6461178399622\n",
      "epoch: 0, step: 23750, loss: 3177.685039270669\n",
      "epoch: 0, step: 23760, loss: 3178.682698663324\n",
      "epoch: 0, step: 23770, loss: 3179.591378118843\n",
      "epoch: 0, step: 23780, loss: 3180.86135103181\n",
      "epoch: 0, step: 23790, loss: 3181.828655537218\n",
      "epoch: 0, step: 23800, loss: 3182.7483934797347\n",
      "epoch: 0, step: 23810, loss: 3183.937840539962\n",
      "epoch: 0, step: 23820, loss: 3185.384482022375\n",
      "epoch: 0, step: 23830, loss: 3186.816599790007\n",
      "epoch: 0, step: 23840, loss: 3187.9117295257747\n",
      "epoch: 0, step: 23850, loss: 3188.9451637528837\n",
      "epoch: 0, step: 23860, loss: 3190.0437628366053\n",
      "epoch: 0, step: 23870, loss: 3191.0510324798524\n",
      "epoch: 0, step: 23880, loss: 3192.3265855647624\n",
      "epoch: 0, step: 23890, loss: 3193.2574101425707\n",
      "epoch: 0, step: 23900, loss: 3194.2972629778087\n",
      "epoch: 0, step: 23910, loss: 3195.460017260164\n",
      "epoch: 0, step: 23920, loss: 3196.9210206754506\n",
      "epoch: 0, step: 23930, loss: 3198.183225709945\n",
      "epoch: 0, step: 23940, loss: 3199.617540102452\n",
      "epoch: 0, step: 23950, loss: 3200.846987154335\n",
      "epoch: 0, step: 23960, loss: 3201.9310038499534\n",
      "epoch: 0, step: 23970, loss: 3204.325120370835\n",
      "epoch: 0, step: 23980, loss: 3206.4403339512646\n",
      "epoch: 0, step: 23990, loss: 3208.0289035849273\n",
      "epoch: 0, step: 24000, loss: 3209.043722745031\n",
      "epoch: 0, step: 24010, loss: 3210.0709738768637\n",
      "epoch: 0, step: 24020, loss: 3211.2151244618\n",
      "epoch: 0, step: 24030, loss: 3212.7155157364905\n",
      "epoch: 0, step: 24040, loss: 3213.7405991367996\n",
      "epoch: 0, step: 24050, loss: 3215.3719899319112\n",
      "epoch: 0, step: 24060, loss: 3216.3005832247436\n",
      "epoch: 0, step: 24070, loss: 3217.2776498384774\n",
      "epoch: 0, step: 24080, loss: 3218.6771766878664\n",
      "epoch: 0, step: 24090, loss: 3219.575344737619\n",
      "epoch: 0, step: 24100, loss: 3220.870569627732\n",
      "epoch: 0, step: 24110, loss: 3222.2049115635455\n",
      "epoch: 0, step: 24120, loss: 3223.663648556918\n",
      "epoch: 0, step: 24130, loss: 3224.4823482967913\n",
      "epoch: 0, step: 24140, loss: 3225.3963058032095\n",
      "epoch: 0, step: 24150, loss: 3226.500082280487\n",
      "epoch: 0, step: 24160, loss: 3227.6120143942535\n",
      "epoch: 0, step: 24170, loss: 3231.600600641221\n",
      "epoch: 0, step: 24180, loss: 3233.079961616546\n",
      "epoch: 0, step: 24190, loss: 3234.5092911459506\n",
      "epoch: 0, step: 24200, loss: 3236.6165544874966\n",
      "epoch: 0, step: 24210, loss: 3237.736206676811\n",
      "epoch: 0, step: 24220, loss: 3238.988714952022\n",
      "epoch: 0, step: 24230, loss: 3240.2195044495165\n",
      "epoch: 0, step: 24240, loss: 3241.1608082838356\n",
      "epoch: 0, step: 24250, loss: 3242.5329753793776\n",
      "epoch: 0, step: 24260, loss: 3243.5547059588134\n",
      "epoch: 0, step: 24270, loss: 3245.5228770561516\n",
      "epoch: 0, step: 24280, loss: 3247.0779013223946\n",
      "epoch: 0, step: 24290, loss: 3248.2582182995975\n",
      "epoch: 0, step: 24300, loss: 3250.0985187478364\n",
      "epoch: 0, step: 24310, loss: 3251.040702071041\n",
      "epoch: 0, step: 24320, loss: 3252.1114219538867\n",
      "epoch: 0, step: 24330, loss: 3253.1158339343965\n",
      "epoch: 0, step: 24340, loss: 3254.301229868084\n",
      "epoch: 0, step: 24350, loss: 3255.732718657702\n",
      "epoch: 0, step: 24360, loss: 3256.688344027847\n",
      "epoch: 0, step: 24370, loss: 3257.898961160332\n",
      "epoch: 0, step: 24380, loss: 3259.2762505970895\n",
      "epoch: 0, step: 24390, loss: 3260.726142730564\n",
      "epoch: 0, step: 24400, loss: 3261.9171939603984\n",
      "epoch: 0, step: 24410, loss: 3262.834677595645\n",
      "epoch: 0, step: 24420, loss: 3263.962163385004\n",
      "epoch: 0, step: 24430, loss: 3264.966790471226\n",
      "epoch: 0, step: 24440, loss: 3266.011435944587\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# print(outputs)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(labels)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/project/s/shreejoy/nxu/envs/patch_seq_spl/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/s/shreejoy/nxu/envs/patch_seq_spl/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/s/shreejoy/nxu/envs/patch_seq_spl/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "n_epochs = 40\n",
    "nucleotide_loader = NtDataset(training_dataset_subset)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "splice_model = SpliceFormer(vocab_size=4, \n",
    "                            model_dim=64, \n",
    "                            n_attn_heads=2, \n",
    "                            n_encoder_layers=2, \n",
    "                            hidden_act=nn.ReLU(), \n",
    "                            dropout=0.1)\n",
    "\n",
    "optimizer = torch.optim.SGD(splice_model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# training loop \n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for seq_number, nucleotide_seq in enumerate(nucleotide_loader):\n",
    "        inputs, labels = nucleotide_seq\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # error is here!\n",
    "        outputs = splice_model(inputs)\n",
    "        # print(outputs)\n",
    "        # print(labels)\n",
    "\n",
    "        total_loss = loss_fn(outputs, labels)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "        if seq_number % 1000 == 0:\n",
    "            print(f'epoch: {epoch}, step: {seq_number}, loss: {running_loss}')\n",
    "            torch.save(splice_model.state_dict(), f'./tbh_model_{seq_number}.pth')\n",
    "            # running_loss = 0.0\n",
    "\n",
    "            \n",
    "print(\"Finished training!\\nFinal loss value:\", total_loss)\n",
    "torch.save(splice_model.state_dict(), './tbh_model_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "618b7f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 3]) torch.Size([80])\n",
      "tensor(1.2545, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for seq_number, nucleotide_seq in enumerate(nucleotide_loader):\n",
    "    inputs, labels = nucleotide_seq\n",
    "    labels = labels.reshape(-1)\n",
    "    # labels = labels.unsqueeze(0)\n",
    "    break\n",
    "\n",
    "outputs = splice_model(inputs)\n",
    "labels = labels.type(torch.LongTensor)\n",
    "# outputs = outputs.unsqueeze(0)\n",
    "print(outputs.shape, labels.shape)\n",
    "total_loss = loss_fn(outputs, labels)\n",
    "print(total_loss)\n",
    "# print(outputs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd200d59-6665-487f-80ff-b8e292d97c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "nucleotide_test_loader = NtDataset(test_dataset)\n",
    "splice_test = SpliceFormer()\n",
    "splice_test.load_state_dict(torch.load('./tbh_model_final.pth', weights_only = True)) \n",
    "\n",
    "outputs = splice_test.(nucleotide_seq)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
